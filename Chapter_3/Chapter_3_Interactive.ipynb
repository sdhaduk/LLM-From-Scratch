{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99afe072-906b-49cf-9c8d-a85a6dd17d73",
   "metadata": {},
   "source": [
    "# Chapter 3 - Interactive\n",
    "\n",
    "### This notebook will contain code blocks, images, and gifs to further enhance your understanding and intuition of specific topics listed below:\n",
    "\n",
    "- #### attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61eeb99-c45f-431b-8be8-7b79d58c17a3",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "#### Instead of processing a word in isolation or in a fixed window, attention allows the model to:\n",
    "\n",
    "* #### dynamically focus on other words in the input sequence\n",
    "\n",
    "* #### weigh the importance of those words based on context\n",
    "\n",
    "* #### capture long-range dependencies, even between words far apart in the text\n",
    "\n",
    "\n",
    "### For example, in the sentence:\n",
    "#### \"The cat, which had been hiding under the couch, finally emerged.\"\n",
    "\n",
    "#### When predicting the word \"emerged\", the model may attend more to \"cat\" than to \"couch\", even though \"cat\" is far away. This is only possible because of attention.\n",
    "\n",
    "#### An example is shown below:\n",
    "\n",
    "<div style=\"max-width:700px\">\n",
    "    \n",
    "![](images/interactive_1.gif)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ebf131-4e6f-44ef-9a90-a4ae3028e596",
   "metadata": {},
   "source": [
    "#### Below is an example of calculating the context vector for a token in a sequence.\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/interactive_3.gif)\n",
    "\n",
    "</div>\n",
    "\n",
    "#### Now lets get into the difference between single-head and multi-head attention.\n",
    "\n",
    "#### Single-head attention:\n",
    "\n",
    "#### In single-head attention, each token uses a single query vector to compare against all key vectors. It produces a single set of attention scores, which are then softmaxed into attention weights. These weights are used to compute a single context vector as a weighted sum of the value vectors. This process helps the model focus on the most relevant parts of the input, but it only attends from a single representation subspace. Limitation: Single-head attention can miss important contextual relationships because it only uses one perspective or feature space to calculate attention.\n",
    "\n",
    "#### Multi-head attention:\n",
    "\n",
    "#### Multi-head attention overcomes the limitations of single-head attention by projecting the same token into multiple sets of query, key, and value spaces (via learned weight matrices). Each head computes its own attention scores and context vector independently. Each head can focus on different types of relationships—e.g., syntactic structure in one head, and semantic similarity in another. After computing each head’s context vector, they are concatenated (or averaged) to form the final representation. This gives the model a richer understanding of context.\n",
    "\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/interactive_2.gif)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23fe716b-4214-44d0-b3b9-b0562899f08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SINGLE-HEAD ATTENTION ===\n",
      "Query: [1 0]\n",
      "Attention scores: [1 1]\n",
      "Softmaxed weights: [0.5 0.5]\n",
      "Context vector (single-head): [1. 1.]\n",
      "\n",
      "=== MULTI-HEAD ATTENTION (2 heads on same token) ===\n",
      "Original token embedding: [1 1]\n",
      "\n",
      "Head 1:\n",
      "  q1 = token_embedding @ W_q1 = [1 1]\n",
      "  Attention weights: [0.73105858 0.26894142]\n",
      "  Context vector: [1.46211716 0.53788284]\n",
      "Head 2:\n",
      "  q2 = token_embedding @ W_q2 = [ 2.  -0.5]\n",
      "  Attention weights: [0.37754067 0.62245933]\n",
      "  Context vector: [0.75508134 1.24491866]\n",
      "\n",
      "Final Multi-head Context (concatenated): [1.46211716 0.53788284 0.75508134 1.24491866]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # numerical stability\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# === SINGLE-HEAD ATTENTION ===\n",
    "q = np.array([1, 0])  # query for the token\n",
    "keys = np.array([[1, 1], [1, 0]])\n",
    "values = np.array([[2, 0], [0, 2]])\n",
    "\n",
    "attn_scores = q @ keys.T  # [q · k1, q · k2]\n",
    "attn_weights = softmax(attn_scores)\n",
    "context_vector = attn_weights @ values\n",
    "\n",
    "print(\"=== SINGLE-HEAD ATTENTION ===\")\n",
    "print(f\"Query: {q}\")\n",
    "print(f\"Attention scores: {attn_scores}\")\n",
    "print(f\"Softmaxed weights: {attn_weights}\")\n",
    "print(f\"Context vector (single-head): {context_vector}\\n\")\n",
    "\n",
    "# === MULTI-HEAD ATTENTION ===\n",
    "token_embedding = np.array([1, 1])  # original embedding for a single token\n",
    "print(\"=== MULTI-HEAD ATTENTION (2 heads on same token) ===\")\n",
    "print(f\"Original token embedding: {token_embedding}\\n\")\n",
    "\n",
    "# --- Head 1 ---\n",
    "W_q1 = np.array([[1, 0], [0, 1]])  # Identity\n",
    "q1 = token_embedding @ W_q1\n",
    "attn_scores_1 = q1 @ keys.T\n",
    "attn_weights_1 = softmax(attn_scores_1)\n",
    "context_vector_1 = attn_weights_1 @ values\n",
    "\n",
    "print(\"Head 1:\")\n",
    "print(f\"  q1 = token_embedding @ W_q1 = {q1}\")\n",
    "print(f\"  Attention weights: {attn_weights_1}\")\n",
    "print(f\"  Context vector: {context_vector_1}\")\n",
    "\n",
    "# --- Head 2 ---\n",
    "W_q2 = np.array([[2, -1], [0, 0.5]])\n",
    "q2 = token_embedding @ W_q2\n",
    "attn_scores_2 = q2 @ keys.T\n",
    "attn_weights_2 = softmax(attn_scores_2)\n",
    "context_vector_2 = attn_weights_2 @ values\n",
    "\n",
    "print(\"Head 2:\")\n",
    "print(f\"  q2 = token_embedding @ W_q2 = {q2}\")\n",
    "print(f\"  Attention weights: {attn_weights_2}\")\n",
    "print(f\"  Context vector: {context_vector_2}\")\n",
    "\n",
    "# Final output\n",
    "multihead_output = np.concatenate([context_vector_1, context_vector_2])\n",
    "print(f\"\\nFinal Multi-head Context (concatenated): {multihead_output}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
