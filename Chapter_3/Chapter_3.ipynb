{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "565a3317-8422-446c-9b37-0c602fbc8914",
   "metadata": {},
   "source": [
    "# Chapter 3: Coding Attention Mechanisms\n",
    "* Will implement four different variations of the attention mechanisms that will build upon eachother, the goal is to arrive at a compact and efficient implementation of multi-head attention\n",
    "\n",
    "1. Simplified self-attention -> simplified version of self-attention before adding trainable weights\n",
    "2. Self-attention -> self attention with the trainable weights\n",
    "3. Casual attention -> adds a mask to self-attention that allows the LLM to generate one word at a time\n",
    "4. Multi-head attention -> organizes attention and allows model to capture various aspects of the input data in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b720d7-d5f0-40ae-a80d-a8ac360b9942",
   "metadata": {},
   "source": [
    "### 3.3.1 Simple self-attention mechanism without trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "845393c7-33e1-425a-aeb7-dc27e4524bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "inputs = torch.tensor(\n",
    "   [[0.43, 0.15, 0.89], # Your     \n",
    "    [0.55, 0.87, 0.66], # journey  \n",
    "    [0.57, 0.85, 0.64], # starts    \n",
    "    [0.22, 0.58, 0.33], # with\n",
    "    [0.77, 0.25, 0.10], # one\n",
    "    [0.05, 0.80, 0.55]] # step\n",
    ")\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac3cfbab-c8d8-4274-83c3-b9cae29598c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omega 2,0: 0.9544000625610352\n",
      "omega 2,1: 1.4950001239776611\n",
      "omega 2,2: 1.4754000902175903\n",
      "omega 2,3: 0.8434000015258789\n",
      "omega 2,4: 0.7070000171661377\n",
      "omega 2,5: 1.0865000486373901\n"
     ]
    }
   ],
   "source": [
    "# Compute attention scores (omega), between the query and all other inputs elements as a dot product\n",
    "\n",
    "query = inputs[1]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "\n",
    "for i, x_i, in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "    print(f\"omega 2,{i}: {attn_scores_2[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2b1857f-5365-4c2f-8f65-cbc97d2d7feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum:  tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# Normalize these attention scores and obtain attention weights (alpha) that sum to 1\n",
    "\n",
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "print(\"Attention weights: \", attn_weights_2_tmp)\n",
    "print(\"Sum: \", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "763ded96-e81a-4116-931b-c1737df3cfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Softmax is better for normalizing values\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(\"Attention weights: \", attn_weights_2_naive)\n",
    "print(\"Sum: \", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f91450e-d874-4cc9-95b0-46ce512b9f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Just use the PyTorch implementation of softmax which has been optimized for performance\n",
    "\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights: \", attn_weights_2)\n",
    "print(\"Sum: \", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5707885a-e57c-4a30-ab05-18a1749085a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "# Now, compute context vector z(2), a combination of all input vectors weighted by the attention weights\n",
    "query = inputs[1]\n",
    "context_vector = torch.zeros(query.shape)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vector += attn_weights_2[i] * x_i\n",
    "    \n",
    "print(context_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42e04b1-0459-40a0-86f7-7b6d9e10cec2",
   "metadata": {},
   "source": [
    "### 3.3.2 Computing attention weights for all input tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d87da8c4-2d54-4c5a-8233-e7a3b2b67b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb345cf1-e3cd-4775-9654-3bb0b90ff5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# forloops are slow, use matrix multiplication\n",
    "\n",
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69b71091-d3ad-4036-affb-604687c0aa90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87b5d42a-9333-4565-b8dd-d737a308b6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13ec02f-a355-410b-a2b8-bd721cc896e3",
   "metadata": {},
   "source": [
    "## 3.4 Implementing self attention with trainable weights\n",
    "* Now we add trainable weights to this attention mechanism through weight matrices.\n",
    "* These weight matrices are updated through training so the model can learn to produce \"good\" context vectors\n",
    "\n",
    "We will tackle this self-attention mechanism in two steps:\n",
    "1. First, code it step-by-step like before\n",
    "2. Second, organize it into a class that can be imported into the LLM architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdfce24-f158-4230-ad77-45924a87271c",
   "metadata": {},
   "source": [
    "### 3.4.1 Computing attention weights step by step\n",
    "* Introduce weight vectors W<sub>q</sub>, W<sub>k</sub>, W<sub>v</sub>\n",
    "* These three matrices are used to project the embedded input token x<sup>(i)</sup>, into query, key, and value vectors, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2a09449-0948-4ace-b54b-be4516e52f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5500, 0.8700, 0.6600])\n"
     ]
    }
   ],
   "source": [
    "x_2 = inputs[1] # second input element\n",
    "d_in = inputs.shape[1] # the input embedding size, d=3\n",
    "d_out = 2 # the output embedding size, d_out=2\n",
    "\n",
    "print(x_2)\n",
    "\n",
    "# note: in GPT-like models, usually d_in and d_out are the same size, but to better calculate the computations we choose d_out=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ab94fc1-4be0-4fa8-a08c-513ecf004191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "# initialize the weight matrices\n",
    "\n",
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d5c0664-ef34-45c1-aaf1-7cc2cbe35b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba196235-4351-4c4e-8b86-4f8570d4c9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "# The attention scores are computed as a dot product between the query and key vectors, since we are trying to compute the context vector for the second input, the query is derived from that input token\n",
    "\n",
    "# comput attention score omega22\n",
    "keys_2 = keys[1]\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04e98ecc-09a0-4fcf-bcb3-4bb37c5593fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "# generalize to all attention scores via matrix mult\n",
    "\n",
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2dd6f15a-9e29-4557-9f4c-5ba9d9353811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "# Now scale the attention scores by dividing them by the square root of the embedding dimension of the keys, then use softmax\n",
    "\n",
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ed9969c-4423-4029-97ba-3c5bd05674b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "# last step is to multiply each value vector with its respective attention weight and then summing them to obtain the context vector\n",
    "\n",
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53933ff7-e8c3-4eb4-a17d-51c144c3b5b6",
   "metadata": {},
   "source": [
    "### 3.4.2 Implementing a compact self-attention Python class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31c34dd3-e8a6-42a1-9749-5e0c812fafbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "670bd0dd-cde2-47dd-9e02-dd6456356984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b00a8741-3f86-4aa3-abc5-e0416140352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can improve our implementation by using nn.Linear layers instead of nn.Parameter\n",
    "\n",
    "import torch.nn as nn\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qvk_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qvk_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qvk_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qvk_bias)\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2189c2ac-a100-4956-bf1a-b2cf4408ce65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e00809-a15a-4039-a14b-6d3857e806eb",
   "metadata": {},
   "source": [
    "####################### Exercise 3.1 #############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50b96c35-4884-41fa-a4a6-6b80e2b0bb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_v1_exercise = SelfAttention_v1(d_in, d_out)\n",
    "sa_v1_exercise.W_query = torch.nn.Parameter(sa_v2.W_query.weight.T)\n",
    "sa_v1_exercise.W_key = torch.nn.Parameter(sa_v2.W_key.weight.T)\n",
    "sa_v1_exercise.W_value = torch.nn.Parameter(sa_v2.W_value.weight.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e40b31d-5083-4d26-8971-a8baa102afe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(sa_v1_exercise(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79f08a6-4ac8-4a7c-a470-be04e215f6d9",
   "metadata": {},
   "source": [
    "#############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3d6392-a75d-460f-bd6e-733e5e24232c",
   "metadata": {},
   "source": [
    "## 3.5 Hiding future words with casual attention\n",
    "* You want the self-attention mechanism to consider only the tokens that appear prior to the current position when predicting the next token in a sequence -> this is called \"causual attention\" or \"masked attention\"\n",
    "* Contrasts the self-attention we implemented above which gives access to the entire input sequence at once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e652ad26-d44e-43a6-828a-d26be9898912",
   "metadata": {},
   "source": [
    "### 3.5.1 Applying a casual attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33bc1be9-8e14-4b4a-a058-96ac47d2c1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d67763e2-7f25-4c27-9916-c40254b571a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# use the PyTorch tril function to create a mask\n",
    "\n",
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7cfe2d37-2100-492b-8908-a8a8ca761ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights * mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff9f6075-b60e-4d1b-bcc3-3bc02c170f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921],\n",
      "        [0.3700],\n",
      "        [0.5357],\n",
      "        [0.6775],\n",
      "        [0.8415],\n",
      "        [1.0000]], grad_fn=<SumBackward1>)\n",
      "\n",
      "\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "print(row_sums)\n",
    "print(\"\\n\")\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d70c4dc-15bf-40a9-9513-b7367d52c05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]]) \n",
      "\n",
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# can implement the computation of the masked attention weights more efficiently in fewer steps\n",
    "\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "print(mask,\"\\n\")\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf) \n",
    "# masked_fill fills elements of self tensor with value where mask is True \n",
    "# .bool() replaces values in tensor that are 0 with False, and 1 with True\n",
    "# where 0's (False) are in the mask, it will keep original values, where 1's (True) are in the mask, will replace with -inf\n",
    "\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3fffa325-1041-41e7-9693-1f33af480d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa0ef42-195a-47c8-83a2-d5eb7e284184",
   "metadata": {},
   "source": [
    "### 3.5.2 Masking additional attention weights with dropout\n",
    "* <b>Dropout</b> is a deep learning technique where randomly selected hidden layer units are ignored during training, effectively \"dropping\" them out\n",
    "* This method helps overfitting by ensuring that a model does not become overly reliant on any specific set of hidden layer units\n",
    "* Only done during training and disabled afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "08db4095-f5dc-4080-9802-b2aea212d882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# implementing dropout mechanism after calculating the attention weights, and using a dropout rate of 50%\n",
    "\n",
    "# first to a 6x6 tensor of 1's for simplicity\n",
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "\n",
    "'''\n",
    "with a dropout rate (p) of 50%, half of the elements in the matrix are randomly set to zero. To comepnsate for the reduction in active elements, \n",
    "the values of the remaining elemnts in the matrix are scaled up by a factor of (1 / 1 - p)\n",
    "'''\n",
    "\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "64f97f58-1111-430a-aa56-810d3cc54b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def52839-cef9-43d4-b896-9e44908e35a5",
   "metadata": {},
   "source": [
    "### 3.5.3 Implementing a compact casual attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cb8bff50-f3d8-4556-8fac-823604a8eb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d3035679-ae18-46c8-a67e-fbcd8e331263",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "            b, num_tokens, d_in = x.shape\n",
    "            keys = self.W_key(x)\n",
    "            queries = self.W_query(x)\n",
    "            values = self.W_value(x)\n",
    "\n",
    "            attn_scores = queries @ keys.transpose(1, 2)\n",
    "            attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "            attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "            attn_weights = self.dropout(attn_weights)\n",
    "            context_vec = attn_weights @ values\n",
    "            return context_vec\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "030e8e0b-10f4-4fa2-ab21-b15e078a4b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CasualAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e2df51-67e7-4205-b318-6a729a17d567",
   "metadata": {},
   "source": [
    "## 3.6 Extending single-head attention to multi-head attention\n",
    "* \"multi-head\" refers to dividing the attention mechanism into multiple \"heads\", each operating independently\n",
    "* In this context, a single casual attention module can be considered single-head attention, where there is only one set of attention weights processing the inputs sequentially"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11f89b8-7ff6-4c16-98bf-91cce380cc3d",
   "metadata": {},
   "source": [
    "### 3.6.1 Stacking multiple single-head attention layers\n",
    "* In practical terms, implementing multi-head attention involves creating multiple instances of the self-attention mechanism, each with its own weights, and then combining their outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e5e3ffaa-3985-4dad-aa3d-fc69093bea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([CasualAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)])\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b652b8f2-aa39-4868-914e-69569965df5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "\n",
    "'''\n",
    "The first dimension of context_vecs is 2 since there are two input texts in the batch\n",
    "The second dimension refers to the 6 input tokens in each input\n",
    "The third dimension refers to the four-dimensional embedding of each token\n",
    "'''\n",
    "\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b62b985-394a-4c26-9bbb-40353c54ee2f",
   "metadata": {},
   "source": [
    "### 3.6.2 Implementing multi-head attention with weight splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0a119b64-72aa-4a5d-adb4-5296a8e758c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert(d_out % num_heads == 0), \\\n",
    "        \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduces the projection dim to match the desired output dim\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out) # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape \n",
    "        keys = self.W_key(x)            # shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)        # ^\n",
    "        values = self.W_value(x)         # ^\n",
    "\n",
    "        print(\"keys.shape:\", keys.shape)\n",
    "        \n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        \n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        print(\"queries:\", queries.shape)  # (b, A, num_tokens, d_v)\n",
    "        print(\"attn_scores:\", attn_scores.shape)  # (b, A, num_tokens, num_tokens)\n",
    "        print(\"context_vec before out_proj:\", context_vec.shape)  # (b, num_tokens, d_out)\n",
    "        \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "51cdf741-e3c1-490a-a553-cc529453f719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch.shape: torch.Size([2, 6, 3]) \n",
      "\n",
      "keys.shape: torch.Size([2, 6, 9])\n",
      "queries: torch.Size([2, 3, 6, 3])\n",
      "attn_scores: torch.Size([2, 3, 6, 6])\n",
      "context_vec before out_proj: torch.Size([2, 6, 9])\n",
      "\n",
      "\n",
      "context_vecs.shape: torch.Size([2, 6, 9]) \n",
      "\n",
      "tensor([[[-0.3019,  0.3177,  0.4985, -0.0611, -0.1675,  0.3447,  0.3815,\n",
      "           0.4080, -0.3164],\n",
      "         [-0.3411,  0.3692,  0.5127, -0.0264,  0.0238,  0.2869,  0.4995,\n",
      "           0.3322, -0.3014],\n",
      "         [-0.3538,  0.3897,  0.5139, -0.0094,  0.0937,  0.2692,  0.5391,\n",
      "           0.3010, -0.2925],\n",
      "         [-0.3461,  0.3929,  0.4836,  0.0283,  0.1188,  0.2660,  0.5234,\n",
      "           0.2770, -0.2561],\n",
      "         [-0.3386,  0.3733,  0.4685,  0.0781,  0.1546,  0.2679,  0.5007,\n",
      "           0.2599, -0.2222],\n",
      "         [-0.3399,  0.3842,  0.4626,  0.0693,  0.1497,  0.2634,  0.5065,\n",
      "           0.2580, -0.2231]],\n",
      "\n",
      "        [[-0.3019,  0.3177,  0.4985, -0.0611, -0.1675,  0.3447,  0.3815,\n",
      "           0.4080, -0.3164],\n",
      "         [-0.3411,  0.3692,  0.5127, -0.0264,  0.0238,  0.2869,  0.4995,\n",
      "           0.3322, -0.3014],\n",
      "         [-0.3538,  0.3897,  0.5139, -0.0094,  0.0937,  0.2692,  0.5391,\n",
      "           0.3010, -0.2925],\n",
      "         [-0.3461,  0.3929,  0.4836,  0.0283,  0.1188,  0.2660,  0.5234,\n",
      "           0.2770, -0.2561],\n",
      "         [-0.3386,  0.3733,  0.4685,  0.0781,  0.1546,  0.2679,  0.5007,\n",
      "           0.2599, -0.2222],\n",
      "         [-0.3399,  0.3842,  0.4626,  0.0693,  0.1497,  0.2634,  0.5065,\n",
      "           0.2580, -0.2231]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "print(\"batch.shape:\", batch.shape, \"\\n\")\n",
    "d_out = 9\n",
    "mha = MultiHeadAttention(d_in, d_out,context_length, 0.0, num_heads=3)\n",
    "context_vecs = mha(batch)\n",
    "print(\"\\n\")\n",
    "print(\"context_vecs.shape:\", context_vecs.shape, \"\\n\")\n",
    "print(context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d603c84-2f77-42e4-81a0-cbfeb56c623f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
