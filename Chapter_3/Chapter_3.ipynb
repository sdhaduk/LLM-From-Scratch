{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "565a3317-8422-446c-9b37-0c602fbc8914",
   "metadata": {},
   "source": [
    "# Chapter 3: Coding Attention Mechanisms\n",
    "\n",
    "#### In this chapter we look at an integral part of the LLM architecture itself, attention mechanisms.\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/3.0_1.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "#### We will implement four different variants of attention mechanisms as shown below. These different variants will build upon eachother, and the goal is to arrive at the compact and efficient implementation of multi-head attention that we can plug into the LLM architecture we will code in the next chapter.\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/3.1_1.png)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b720d7-d5f0-40ae-a80d-a8ac360b9942",
   "metadata": {},
   "source": [
    "## 3.1 The problem with modeling long sequences\n",
    "\n",
    "#### Lets consider the problem with pre-LLM architectures that do not have attention mechanisms. If we wanted to develop a translation model that translates text from one language into another, we cant simply translate word by word due to the grammatical structures in the source and target language.\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/3.1_2.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "#### Two submodules, an encoder and decoder were used to address this problem. The encoder reads and processed the text, the decoder then produces the translated text.\n",
    "\n",
    "#### The recurrent neural networks (RNNs) were the most popular encoder-decorder architecture for language translation before transformers. While we don't need to know the inner workings of these encoder-decoder RNNs, the key idea is that the encoder part processes the entire input text into a hidden state (memory cell). The decoder then takes in this hidden state to produce the output. The embedding vector can be analgous to this hidden state.\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/3.1_3.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "#### The big limitation is that the RNN cannot directly access the earlier hidden states from the encoder during the deocoding process. This means the decoder relies soley on the final hidden state from the encoder, which is supposed to encapsulate all the relevent information. This leads to a large loss of context, especially in complex setnences were dependencies might span long distances in the text. \n",
    "\n",
    "#### Just remember that encoder-decoder RNNs had a shortcoming that motivated the design of attention mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e49af3-f1bd-4fd4-b63c-f7db9310f13e",
   "metadata": {},
   "source": [
    "## 3.2 Capturing data dependencies with attention mechanisms\n",
    "\n",
    "#### Self-attention is a mechanism that allows each position in the input sequence to consider the relevancy of all other positions in the same sequence when computing the representation of a sequence.\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/3.2_1.png)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b097f8-9bfb-4d16-9ed7-f510dc72616b",
   "metadata": {},
   "source": [
    "## 3.3 Attending to different parts of the input with self-attention\n",
    "\n",
    "#### Self-attention can seem complex, so we will begin by examining a simplified version of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674de73d-9bde-4ef9-8d56-418bcac353d5",
   "metadata": {},
   "source": [
    "### 3.3.1 A simple self-attention mechanism without trainable weights\n",
    "\n",
    "#### The goal of self-attention is to compute a context vector for each input element that combines information from all other input elements. In the image below, we calculate context vector z<sup>(2)</sup>. The contribution of each input element for computing z<sup>(2)</sup> is determined by attention weights a<sub>21</sub> to a<sub>2T</sub>.\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/3.3_1.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "#### Context vectors are integral to self-attention. They create an enriched representation of each element in an input sequence by incorporating information from all other elements in the sequence. This is important for LLMs, which must understand the relationship and relevancy of words in a sentence to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "845393c7-33e1-425a-aeb7-dc27e4524bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "inputs = torch.tensor(\n",
    "   [[0.43, 0.15, 0.89], # Your     \n",
    "    [0.55, 0.87, 0.66], # journey  \n",
    "    [0.57, 0.85, 0.64], # starts    \n",
    "    [0.22, 0.58, 0.33], # with\n",
    "    [0.77, 0.25, 0.10], # one\n",
    "    [0.05, 0.80, 0.55]] # step\n",
    ")\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea7e9de-b145-4308-90ab-39cea677fb83",
   "metadata": {},
   "source": [
    "#### The first step in implementing self-attention is to compute the attention values, referred to as w. \n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/3.3_2.png)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac3cfbab-c8d8-4274-83c3-b9cae29598c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omega 2,0: 0.9544000625610352\n",
      "omega 2,1: 1.4950001239776611\n",
      "omega 2,2: 1.4754000902175903\n",
      "omega 2,3: 0.8434000015258789\n",
      "omega 2,4: 0.7070000171661377\n",
      "omega 2,5: 1.0865000486373901\n"
     ]
    }
   ],
   "source": [
    "# Compute attention scores (omega), between the query and all other inputs elements as a dot product\n",
    "\n",
    "query = inputs[1]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "\n",
    "for i, x_i, in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "    print(f\"omega 2,{i}: {attn_scores_2[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbdd128-bee4-4721-9f1c-2a98bd95c34e",
   "metadata": {},
   "source": [
    "#### In the text step, we normalize each of these computed attention scores. The goal behind this normalization is to obtain attention weights that sum to 1, which is useful for interpretation and mainting training stability.\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/3.3_3.png)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2b1857f-5365-4c2f-8f65-cbc97d2d7feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum:  tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# Normalize these attention scores and obtain attention weights (alpha) that sum to 1\n",
    "\n",
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "print(\"Attention weights: \", attn_weights_2_tmp)\n",
    "print(\"Sum: \", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "763ded96-e81a-4116-931b-c1737df3cfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Softmax is better for normalizing values\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(\"Attention weights: \", attn_weights_2_naive)\n",
    "print(\"Sum: \", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f91450e-d874-4cc9-95b0-46ce512b9f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Just use the PyTorch implementation of softmax which has been optimized for performance\n",
    "\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights: \", attn_weights_2)\n",
    "print(\"Sum: \", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c145200d-3c3c-4c29-87cd-114f31276cdf",
   "metadata": {},
   "source": [
    "#### After computing the attention weights, we calculate the context vector z<sup>(2)</sup> by multiplying embedded input tokens, x(i), with the corresponding attention weights and then summing the resulting vectors. Thus, context vector z<sup>(2)</sup> is the weighted sum of all input vectors.\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/3.3_4.png)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5707885a-e57c-4a30-ab05-18a1749085a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "# Now, compute context vector z(2), a combination of all input vectors weighted by the attention weights\n",
    "query = inputs[1]\n",
    "context_vector = torch.zeros(query.shape)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vector += attn_weights_2[i] * x_i\n",
    "    \n",
    "print(context_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42e04b1-0459-40a0-86f7-7b6d9e10cec2",
   "metadata": {},
   "source": [
    "### 3.3.2 Computing attention weights for all input tokens\n",
    "\n",
    "#### Lets generalize this procedure for computing context vectors to calculate all context vectors simultaneously.\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/3.3_5.png)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57c49ec7-747c-4560-a6bd-84534310be79",
   "metadata": {},
   "source": [
    "#### We follow the same three steps, but we make some modifications to compute all vectors instead of only one.\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/3.3_6.png)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d87da8c4-2d54-4c5a-8233-e7a3b2b67b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0b5b4a-b9de-460c-b6a4-f3a8673b010c",
   "metadata": {},
   "source": [
    "#### Each element in the tensor are an attention score between each pair of inputs. In the code above we used a forloop, but those are slow. The same calculation can be made much faster with matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb345cf1-e3cd-4775-9654-3bb0b90ff5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# forloops are slow, use matrix multiplication\n",
    "\n",
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69b71091-d3ad-4036-affb-604687c0aa90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87b5d42a-9333-4565-b8dd-d737a308b6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13ec02f-a355-410b-a2b8-bd721cc896e3",
   "metadata": {},
   "source": [
    "## 3.4 Implementing self attention with trainable weights\n",
    "\n",
    "#### The next step is to implement the self-attention mechanism used in the original transformer architecture. This self-attention mechanism is called the <i>scaled dot-product attention</i>.\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/3.4_1.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "#### This section will introduce weight matrices athat are updated during model training, these are crucial so that the model can learn to produce \"good\" context vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdfce24-f158-4230-ad77-45924a87271c",
   "metadata": {},
   "source": [
    "### 3.4.1 Computing attention weights step by step\n",
    "\n",
    "#### We introduce three trainable weight matrices, W<sub>q</sub>, W<sub>k</sub>, W<sub>v</sub>. These three matrices are used to project the embedded input tokens, x<sup>(i)</sup> into query, key, and value vectors, respectively.\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/3.4_2.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "#### We start here by computing only one context vector, z<sup>(2)</sup>, then modify this code to calculate all context vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2a09449-0948-4ace-b54b-be4516e52f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5500, 0.8700, 0.6600])\n"
     ]
    }
   ],
   "source": [
    "x_2 = inputs[1] # second input element\n",
    "d_in = inputs.shape[1] # the input embedding size, d=3\n",
    "d_out = 2 # the output embedding size, d_out=2\n",
    "\n",
    "print(x_2)\n",
    "\n",
    "# note: in GPT-like models, usually d_in and d_out are the same size, but to better calculate the computations we choose d_out=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ab94fc1-4be0-4fa8-a08c-513ecf004191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "# initialize the weight matrices\n",
    "\n",
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d5c0664-ef34-45c1-aaf1-7cc2cbe35b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42ad0c2-a38c-4e86-b584-16d26741c149",
   "metadata": {},
   "source": [
    "#### As you can see, we successfully projected the six input tokens from a three-dimensional onto a two-dimensional embedding space. \n",
    "\n",
    "#### The second step is to compute the attention scores, as shown below.\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/3.4_3.png)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba196235-4351-4c4e-8b86-4f8570d4c9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "# The attention scores are computed as a dot product between the query and key vectors, since we are trying to compute the context vector for the second input, the query is derived from that input token\n",
    "\n",
    "# comput attention score omega22\n",
    "keys_2 = keys[1]\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04e98ecc-09a0-4fcf-bcb3-4bb37c5593fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "# generalize to all attention scores via matrix mult\n",
    "\n",
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eab744-19e0-4e50-b69a-f6cd8f051d5c",
   "metadata": {},
   "source": [
    "#### Now we need to go from attention scores to attention weights. We compute the attention weights by scaling the attention scores using softmax. However, in <i>scaled dot-product attention</i>, we scale the attention by dividing them by the square root of the embedding dimension of the keys.\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/3.4_4.png)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2dd6f15a-9e29-4557-9f4c-5ba9d9353811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "# Now scale the attention scores by dividing them by the square root of the embedding dimension of the keys, then use softmax\n",
    "\n",
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf1d0fe-1ca2-4e88-9f4a-55f16f3ffbff",
   "metadata": {},
   "source": [
    "#### We now compute the context vector as a weighted sum over the value vectors, intead of the input vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ed9969c-4423-4029-97ba-3c5bd05674b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "# last step is to multiply each value vector with its respective attention weight and then summing them to obtain the context vector\n",
    "\n",
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53933ff7-e8c3-4eb4-a17d-51c144c3b5b6",
   "metadata": {},
   "source": [
    "### 3.4.2 Implementing a compact self-attention Python class\n",
    "\n",
    "#### We only computed a single context vector z<sup>(2)</sup>. Now we generalize the code to compute all context vectors in the input sequence, z<sup>(1)</sup> to z<sup>(T)</sup>.\n",
    "\n",
    "#### Lets organize this code into a Python class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31c34dd3-e8a6-42a1-9749-5e0c812fafbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5117fda-fea5-47ba-9d97-c01a8387a809",
   "metadata": {},
   "source": [
    "#### The __init__ method initializes trainable weight matrices (W_query, W_key, W_value) for queries, keys, and values, each transforming the input dimension d_in to an output dimension d_out.\n",
    "\n",
    "#### During the forward pass we compute attention scores by multiplying queries and keys, normalizing these scores using softmax. Finally, we create a context vector by weighting the values with these normalized attention scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "670bd0dd-cde2-47dd-9e02-dd6456356984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b00a8741-3f86-4aa3-abc5-e0416140352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can improve our implementation by using nn.Linear layers instead of nn.Parameter\n",
    "\n",
    "import torch.nn as nn\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qvk_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qvk_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qvk_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qvk_bias)\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac2edb5-99ca-4537-9e6d-3482d4fe8cf3",
   "metadata": {},
   "source": [
    "#### Here is a summary of the self-attention mechanism we just implemented:\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/3.4_6.png)\n",
    "\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3d6392-a75d-460f-bd6e-733e5e24232c",
   "metadata": {},
   "source": [
    "## 3.5 Hiding future words with casual attention\n",
    "\n",
    "#### For many LLM tasks, you want the self-attention mechanism to only consider tokens that appear prior to the current position when predicting the next token in a sequence. Causual attention, also known as <i>masked attention</i>, is a specialized form of self-attention. It restricts a model to consider previous and current inputs in a sequence when processing any given token when computing attention scores.\n",
    "\n",
    "#### We will modify the standard self-attention mechanism to create a <i>casual attention</i> mechanism. We mask out the attention weights above the diagonal, and we normalize the nonmasked attention weights such that the attention weights sum to 1 in each row.\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/3.5_1.png)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e652ad26-d44e-43a6-828a-d26be9898912",
   "metadata": {},
   "source": [
    "### 3.5.1 Applying a casual attention mask\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/3.5_2.png)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33bc1be9-8e14-4b4a-a058-96ac47d2c1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d67763e2-7f25-4c27-9916-c40254b571a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# use the PyTorch tril function to create a mask\n",
    "\n",
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7cfe2d37-2100-492b-8908-a8a8ca761ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights * mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff9f6075-b60e-4d1b-bcc3-3bc02c170f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921],\n",
      "        [0.3700],\n",
      "        [0.5357],\n",
      "        [0.6775],\n",
      "        [0.8415],\n",
      "        [1.0000]], grad_fn=<SumBackward1>)\n",
      "\n",
      "\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "print(row_sums)\n",
    "print(\"\\n\")\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5f6d55-0b53-4a89-a282-719ec603febf",
   "metadata": {},
   "source": [
    "#### We can still improve our implementation of causal attention by taking advantage of a property of the softmax function.\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/3.5_3.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "#### When -inf values are present in a row, the softmax function treats them as a zero probability. We can implement this more efficient masking \"trick\" by creating a mask with 1s above the diagonal and then replacing these 1s with -inf values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d70c4dc-15bf-40a9-9513-b7367d52c05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]]) \n",
      "\n",
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# can implement the computation of the masked attention weights more efficiently in fewer steps\n",
    "\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "print(mask,\"\\n\")\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf) \n",
    "# masked_fill fills elements of self tensor with value where mask is True \n",
    "# .bool() replaces values in tensor that are 0 with False, and 1 with True\n",
    "# where 0's (False) are in the mask, it will keep original values, where 1's (True) are in the mask, will replace with -inf\n",
    "\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3fffa325-1041-41e7-9693-1f33af480d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f618edf9-c97e-4f67-b070-e1b795a21c41",
   "metadata": {},
   "source": [
    "#### We have one more minor tweak to the casual attention mechanism that is useful for reducing overfitting when training LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa0ef42-195a-47c8-83a2-d5eb7e284184",
   "metadata": {},
   "source": [
    "### 3.5.2 Masking additional attention weights with dropout\n",
    "\n",
    "#### <b>Dropout</b> in deep learning is where randomly selected hidden layer units are ignored during training, effectively \"dropping\" them out. Dropout is only used during training and is disabled during inference.\n",
    "\n",
    "#### In the transformer architecture, dropout is applied at two specific times: after calculating the attention weights or after applying the attention weights to the value vectors. Here we apply the dropout mask after computing the attention weights.\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/3.5_4.png)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "08db4095-f5dc-4080-9802-b2aea212d882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# implementing dropout mechanism after calculating the attention weights, and using a dropout rate of 50%\n",
    "\n",
    "# first to a 6x6 tensor of 1's for simplicity\n",
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3167cc-b0ba-4960-9e2e-09279c2971ef",
   "metadata": {},
   "source": [
    "#### With a dropout rate (p) of 50%, half of the elements in the matrix are randomly set to zero. To compensate for the reduction in active elements, the values of the remaining elemnts in the matrix are scaled up by a factor of (1 / 1 - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "64f97f58-1111-430a-aa56-810d3cc54b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def52839-cef9-43d4-b896-9e44908e35a5",
   "metadata": {},
   "source": [
    "### 3.5.3 Implementing a compact casual attention class\n",
    "\n",
    "#### We now incorporate the casual attention and dropout modifications to the SelfAttention class. This class will serve as a template for developing <b>multi-head attention</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cb8bff50-f3d8-4556-8fac-823604a8eb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d3035679-ae18-46c8-a67e-fbcd8e331263",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "            b, num_tokens, d_in = x.shape\n",
    "            keys = self.W_key(x)\n",
    "            queries = self.W_query(x)\n",
    "            values = self.W_value(x)\n",
    "\n",
    "            attn_scores = queries @ keys.transpose(1, 2)\n",
    "            attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "            attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "            attn_weights = self.dropout(attn_weights)\n",
    "            context_vec = attn_weights @ values\n",
    "            return context_vec\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "030e8e0b-10f4-4fa2-ab21-b15e078a4b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CasualAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d6b959-5032-4017-b037-3a2f4bf0dbdd",
   "metadata": {},
   "source": [
    "#### The image below summarizes what we have accomplished so far.\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/3.5_5.png)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e2df51-67e7-4205-b318-6a729a17d567",
   "metadata": {},
   "source": [
    "## 3.6 Extending single-head attention to multi-head attention\n",
    "\n",
    "#### The final step is to extend the previously implemented casual attention class over multiple heads. This is known as multi-head attention.\n",
    "\n",
    "#### \"Mutlti-head\" refers to dividing the attention mechanism into multiple \"heads\", each independent. In this context, a single casual attention module can be considered single-head attention, where there is only one set of attention weights processing the input sequentially.\n",
    "\n",
    "#### First, we will build a multi-head attention module by stacking multiple CasualAttention modules. Then we implement the same module in a more computational efficient way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11f89b8-7ff6-4c16-98bf-91cce380cc3d",
   "metadata": {},
   "source": [
    "### 3.6.1 Stacking multiple single-head attention layers\n",
    "\n",
    "#### Implementing multi-head attention involves creating multiple instances of the self-attention mechanism, each with its own weights, and then combining their outputs. \n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/3.6_1.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "#### In the figure above we see that the multi-head attention module includes two single-head attention modules stacked on top of eachother. We now have two value weight matrices, W<sub>v1</sub> and W<sub>v2</sub>. The same applies to the other weight matrices, W<sub>q</sub> and W<sub>k</sub>. We obtain two sets of context vectors Z<sub>1</sub> and Z<sub>2</sub> that we combine into a single context vector matrix Z.\n",
    "\n",
    "#### The main idea is to run the attention mechanism multiple times (in parallel) with different, learned linear projections - the result of multiplying the input data by a weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e5e3ffaa-3985-4dad-aa3d-fc69093bea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([CasualAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)])\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87705a3-b44b-40a6-b4aa-f6c522a5a81e",
   "metadata": {},
   "source": [
    "#### If we use this MultiHeadAttentionWrapper class with two attention heads (num_heads=2) and CasualAttention output dimension d_out=2, we get a four dimensional context vector.\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/3.6_2.png)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b652b8f2-aa39-4868-914e-69569965df5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "\n",
    "'''\n",
    "The first dimension of context_vecs is 2 since there are two input texts in the batch\n",
    "The second dimension refers to the 6 input tokens in each input\n",
    "The third dimension refers to the four-dimensional embedding of each token\n",
    "'''\n",
    "\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3368c8-d6e7-460f-b030-9a1af39ba66a",
   "metadata": {},
   "source": [
    "#### Up to this point, we have implemented a MultiHeadAttentionWrapper that combined multiple single-head attention modules. However, these are processed sequentially via [head(x) for head in self.heads] in the forward method. We can improve this implementation by processing the heads in parallel. One way to achieve this is by computing the outputs for all attention heads simultaneously via matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b62b985-394a-4c26-9bbb-40353c54ee2f",
   "metadata": {},
   "source": [
    "### 3.6.2 Implementing multi-head attention with weight splits\n",
    "\n",
    "#### We can combine MultiHeadAttentionWrapper and CasualAttention classes into one class, MultiHeadAttention. We will also make some other modifications to implement it more efficiently.\n",
    "\n",
    "#### The following class integrates the multi-head functionality by splitting the input into multiple heads by reshaping the projected query, key, and value tensors and then combines the results for these heads after computing attention.\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/3.6_3.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "#### The top is the previous class, and the bottom is what we implement below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0a119b64-72aa-4a5d-adb4-5296a8e758c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert(d_out % num_heads == 0), \\\n",
    "        \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduces the projection dim to match the desired output dim\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out) # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape \n",
    "        keys = self.W_key(x)            # shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)        # ^\n",
    "        values = self.W_value(x)         # ^\n",
    "\n",
    "        print(\"keys.shape:\", keys.shape)\n",
    "        \n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        \n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        print(\"queries:\", queries.shape)  # (b, A, num_tokens, d_v)\n",
    "        print(\"attn_scores:\", attn_scores.shape)  # (b, A, num_tokens, num_tokens)\n",
    "        print(\"context_vec before out_proj:\", context_vec.shape)  # (b, num_tokens, d_out)\n",
    "        \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb94006-556c-468a-bfa3-de30ee601b05",
   "metadata": {},
   "source": [
    "#### The splitting of the query, key and value tensors is achieved through tensor reshaping and transposing operations using PyTorch's .view and .transpose methods. The input is first transformed and then reshaped to represent multiple heads.\n",
    "\n",
    "#### The key operation is to split the d_out dimension into num_heads and head_dim, where head_dim = d_out / num_heads. The splitting is then achieved by using the .view method: a tensor of dimensions (b, num_tokens, d_out) is reshaped to dimension (b, num_tokens, num_heads, head_dim)\n",
    "\n",
    "#### The tensors are then transposed to bring the num_heads dimension before the num_token dimension, resulting in a shape of (b, num_heads, num_tokens, head_dim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "51cdf741-e3c1-490a-a553-cc529453f719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch.shape: torch.Size([2, 6, 3]) \n",
      "\n",
      "keys.shape: torch.Size([2, 6, 9])\n",
      "queries: torch.Size([2, 3, 6, 3])\n",
      "attn_scores: torch.Size([2, 3, 6, 6])\n",
      "context_vec before out_proj: torch.Size([2, 6, 9])\n",
      "\n",
      "\n",
      "context_vecs.shape: torch.Size([2, 6, 9]) \n",
      "\n",
      "tensor([[[-0.3019,  0.3177,  0.4985, -0.0611, -0.1675,  0.3447,  0.3815,\n",
      "           0.4080, -0.3164],\n",
      "         [-0.3411,  0.3692,  0.5127, -0.0264,  0.0238,  0.2869,  0.4995,\n",
      "           0.3322, -0.3014],\n",
      "         [-0.3538,  0.3897,  0.5139, -0.0094,  0.0937,  0.2692,  0.5391,\n",
      "           0.3010, -0.2925],\n",
      "         [-0.3461,  0.3929,  0.4836,  0.0283,  0.1188,  0.2660,  0.5234,\n",
      "           0.2770, -0.2561],\n",
      "         [-0.3386,  0.3733,  0.4685,  0.0781,  0.1546,  0.2679,  0.5007,\n",
      "           0.2599, -0.2222],\n",
      "         [-0.3399,  0.3842,  0.4626,  0.0693,  0.1497,  0.2634,  0.5065,\n",
      "           0.2580, -0.2231]],\n",
      "\n",
      "        [[-0.3019,  0.3177,  0.4985, -0.0611, -0.1675,  0.3447,  0.3815,\n",
      "           0.4080, -0.3164],\n",
      "         [-0.3411,  0.3692,  0.5127, -0.0264,  0.0238,  0.2869,  0.4995,\n",
      "           0.3322, -0.3014],\n",
      "         [-0.3538,  0.3897,  0.5139, -0.0094,  0.0937,  0.2692,  0.5391,\n",
      "           0.3010, -0.2925],\n",
      "         [-0.3461,  0.3929,  0.4836,  0.0283,  0.1188,  0.2660,  0.5234,\n",
      "           0.2770, -0.2561],\n",
      "         [-0.3386,  0.3733,  0.4685,  0.0781,  0.1546,  0.2679,  0.5007,\n",
      "           0.2599, -0.2222],\n",
      "         [-0.3399,  0.3842,  0.4626,  0.0693,  0.1497,  0.2634,  0.5065,\n",
      "           0.2580, -0.2231]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "print(\"batch.shape:\", batch.shape, \"\\n\")\n",
    "d_out = 9\n",
    "mha = MultiHeadAttention(d_in, d_out,context_length, 0.0, num_heads=3)\n",
    "context_vecs = mha(batch)\n",
    "print(\"\\n\")\n",
    "print(\"context_vecs.shape:\", context_vecs.shape, \"\\n\")\n",
    "print(context_vecs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
