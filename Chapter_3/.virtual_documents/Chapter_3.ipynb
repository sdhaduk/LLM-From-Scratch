





import torch
inputs = torch.tensor(
   [[0.43, 0.15, 0.89], # Your     
    [0.55, 0.87, 0.66], # journey  
    [0.57, 0.85, 0.64], # starts    
    [0.22, 0.58, 0.33], # with
    [0.77, 0.25, 0.10], # one
    [0.05, 0.80, 0.55]] # step
)
inputs.shape


# Compute attention scores (omega), between the query and all other inputs elements as a dot product

query = inputs[1]
attn_scores_2 = torch.empty(inputs.shape[0])

for i, x_i, in enumerate(inputs):
    attn_scores_2[i] = torch.dot(x_i, query)
    print(f"omega 2,{i}: {attn_scores_2[i]}")


# Normalize these attention scores and obtain attention weights (alpha) that sum to 1

attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()
print("Attention weights: ", attn_weights_2_tmp)
print("Sum: ", attn_weights_2_tmp.sum())


# Softmax is better for normalizing values
def softmax_naive(x):
    return torch.exp(x) / torch.exp(x).sum(dim=0)

attn_weights_2_naive = softmax_naive(attn_scores_2)
print("Attention weights: ", attn_weights_2_naive)
print("Sum: ", attn_weights_2_naive.sum())


# Just use the PyTorch implementation of softmax which has been optimized for performance

attn_weights_2 = torch.softmax(attn_scores_2, dim=0)
print("Attention weights: ", attn_weights_2)
print("Sum: ", attn_weights_2.sum())


# Now, compute context vector z(2), a combination of all input vectors weighted by the attention weights
query = inputs[1]
context_vector = torch.zeros(query.shape)

for i, x_i in enumerate(inputs):
    context_vector += attn_weights_2[i] * x_i
    
print(context_vector)





attn_scores = torch.empty(6, 6)
for i, x_i in enumerate(inputs):
    for j, x_j in enumerate(inputs):
        attn_scores[i, j] = torch.dot(x_i, x_j)
print(attn_scores)


# forloops are slow, use matrix multiplication

attn_scores = inputs @ inputs.T
print(attn_scores)


attn_weights = torch.softmax(attn_scores, dim=-1)
print(attn_weights)


all_context_vecs = attn_weights @ inputs
print(all_context_vecs)








x_2 = inputs[1] # second input element
d_in = inputs.shape[1] # the input embedding size, d=3
d_out = 2 # the output embedding size, d_out=2

print(x_2)

# note: in GPT-like models, usually d_in and d_out are the same size, but to better calculate the computations we choose d_out=2


# initialize the weight matrices

torch.manual_seed(123)
W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)
W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)
W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)

query_2 = x_2 @ W_query
key_2 = x_2 @ W_key
value_2 = x_2 @ W_value
print(query_2)


keys = inputs @ W_key
values = inputs @ W_value
print("keys.shape:", keys.shape)
print("values.shape:", values.shape)


# The attention scores are computed as a dot product between the query and key vectors, since we are trying to compute the context vector for the second input, the query is derived from that input token

# comput attention score omega22
keys_2 = keys[1]
attn_score_22 = query_2.dot(keys_2)
print(attn_score_22)


# generalize to all attention scores via matrix mult

attn_scores_2 = query_2 @ keys.T
print(attn_scores_2)


# Now scale the attention scores by dividing them by the square root of the embedding dimension of the keys, then use softmax

d_k = keys.shape[-1]
attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)
print(attn_weights_2)


# last step is to multiply each value vector with its respective attention weight and then summing them to obtain the context vector

context_vec_2 = attn_weights_2 @ values
print(context_vec_2)





import torch.nn as nn
class SelfAttention_v1(nn.Module):
    def __init__(self, d_in, d_out):
        super().__init__()
        self.W_query = nn.Parameter(torch.rand(d_in, d_out))
        self.W_key = nn.Parameter(torch.rand(d_in, d_out))
        self.W_value = nn.Parameter(torch.rand(d_in, d_out))
    def forward(self, x):
        keys = x @ self.W_key
        queries = x @ self.W_query
        values = x @ self.W_value
        attn_scores = queries @ keys.T
        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        context_vec = attn_weights @ values
        return context_vec
        


torch.manual_seed(123)
sa_v1 = SelfAttention_v1(d_in, d_out)
print(sa_v1(inputs))


# can improve our implementation by using nn.Linear layers instead of nn.Parameter

import torch.nn as nn
class SelfAttention_v2(nn.Module):
    def __init__(self, d_in, d_out, qvk_bias=False):
        super().__init__()
        self.W_query = nn.Linear(d_in, d_out, bias=qvk_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qvk_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qvk_bias)
    def forward(self, x):
        keys = self.W_key(x)
        queries = self.W_query(x)
        values = self.W_value(x)
        attn_scores = queries @ keys.T
        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        context_vec = attn_weights @ values
        return context_vec


torch.manual_seed(789)
sa_v2 = SelfAttention_v2(d_in, d_out)
print(sa_v2(inputs))





sa_v1_exercise = SelfAttention_v1(d_in, d_out)
sa_v1_exercise.W_query = torch.nn.Parameter(sa_v2.W_query.weight.T)
sa_v1_exercise.W_key = torch.nn.Parameter(sa_v2.W_key.weight.T)
sa_v1_exercise.W_value = torch.nn.Parameter(sa_v2.W_value.weight.T)


print(sa_v1_exercise(inputs))











queries = sa_v2.W_query(inputs)
keys = sa_v2.W_key(inputs)
attn_scores = queries @ keys.T
attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
print(attn_weights)


# use the PyTorch tril function to create a mask

context_length = attn_scores.shape[0]
mask_simple = torch.tril(torch.ones(context_length, context_length))
print(mask_simple)


masked_simple = attn_weights * mask_simple
print(masked_simple)


row_sums = masked_simple.sum(dim=-1, keepdim=True)
print(row_sums)
print("\n")
masked_simple_norm = masked_simple / row_sums
print(masked_simple_norm)


# can implement the computation of the masked attention weights more efficiently in fewer steps

mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)
print(mask,"\n")
masked = attn_scores.masked_fill(mask.bool(), -torch.inf) 
# masked_fill fills elements of self tensor with value where mask is True 
# .bool() replaces values in tensor that are 0 with False, and 1 with True
# where 0's (False) are in the mask, it will keep original values, where 1's (True) are in the mask, will replace with -inf

print(masked)


attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)
print(attn_weights)





# implementing dropout mechanism after calculating the attention weights, and using a dropout rate of 50%

# first to a 6x6 tensor of 1's for simplicity
torch.manual_seed(123)
dropout = torch.nn.Dropout(0.5)
example = torch.ones(6, 6)

'''
with a dropout rate (p) of 50%, half of the elements in the matrix are randomly set to zero. To comepnsate for the reduction in active elements, 
the values of the remaining elemnts in the matrix are scaled up by a factor of (1 / 1 - p)
'''

print(dropout(example))


torch.manual_seed(123)
print(dropout(attn_weights))





batch = torch.stack((inputs, inputs), dim=0)
print(batch.shape)


class CasualAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):
        super().__init__()
        self.d_out = d_out
        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.dropout = nn.Dropout(dropout)
        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))

    def forward(self, x):
            b, num_tokens, d_in = x.shape
            keys = self.W_key(x)
            queries = self.W_query(x)
            values = self.W_value(x)

            attn_scores = queries @ keys.transpose(1, 2)
            attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)
            attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
            attn_weights = self.dropout(attn_weights)
            context_vec = attn_weights @ values
            return context_vec
            


torch.manual_seed(123)
context_length = batch.shape[1]
ca = CasualAttention(d_in, d_out, context_length, 0.0)
context_vecs = ca(batch)
print("context_vecs.shape:", context_vecs.shape)








class MultiHeadAttentionWrapper(nn.Module):
    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
        super().__init__()
        self.heads = nn.ModuleList([CasualAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)])
    def forward(self, x):
        return torch.cat([head(x) for head in self.heads], dim=-1)


torch.manual_seed(123)
context_length = batch.shape[1] # number of tokens
d_in, d_out = 3, 2

mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)
context_vecs = mha(batch)

print(context_vecs)

'''
The first dimension of context_vecs is 2 since there are two input texts in the batch
The second dimension refers to the 6 input tokens in each input
The third dimension refers to the four-dimensional embedding of each token
'''

print("context_vecs.shape:", context_vecs.shape)





class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
        super().__init__()
        assert(d_out % num_heads == 0), \
        "d_out must be divisible by num_heads"

        self.d_out = d_out
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads # Reduces the projection dim to match the desired output dim
        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.out_proj = nn.Linear(d_out, d_out) # Linear layer to combine head outputs
        self.dropout = nn.Dropout(dropout)
        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))

    def forward(self, x):
        b, num_tokens, d_in = x.shape 
        keys = self.W_key(x)            # shape: (b, num_tokens, d_out)
        queries = self.W_query(x)        # ^
        values = self.W_value(x)         # ^
        
        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
        values = values.view(b, num_tokens, self.num_heads, self.head_dim)
        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

        keys = keys.transpose(1, 2)
        queries = queries.transpose(1, 2)
        values = values.transpose(1, 2)

        attn_scores = queries @ keys.transpose(2, 3)
        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]

        attn_scores.masked_fill_(mask_bool, -torch.inf)

        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)
        attn_weights = self.dropout(attn_weights)

        context_vec = (attn_weights @ values).transpose(1, 2)
        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)
        
        context_vec = self.out_proj(context_vec)
        return context_vec


torch.manual_seed(123)
batch_size, context_length, d_in = batch.shape
print("batch.shape:", batch.shape, "\n")
d_out = 2
mha = MultiHeadAttention(d_in, d_out,context_length, 0.0, num_heads=2)
context_vecs = mha(batch)
print(context_vecs)
print("context_vecs.shape:", context_vecs.shape)
