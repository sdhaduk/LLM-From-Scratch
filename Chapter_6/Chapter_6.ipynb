{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00e270db-8bcd-4584-beaf-f3fd6b031631",
   "metadata": {},
   "source": [
    "# Chapter 6: Fine-tuning for classifcation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7243d86-9858-4493-96f1-9ffd490d4343",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    \n",
    "Now we will fine-tune our LLM on a specific target task, such as classifying text.\n",
    "\n",
    "<div style=\"max-width:600px\">\n",
    "    \n",
    "![](images/6.0_1.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "This figure highlights the two main ways of fine-tuning an LLM\n",
    "\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835a8b20-6b18-4889-9119-6b814e724ff0",
   "metadata": {},
   "source": [
    "## 6.1 Different categories of fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d416d639-7a06-42e7-aee8-73e6126f1851",
   "metadata": {},
   "source": [
    "<h4>\n",
    "\n",
    "Two most common ways to fine-tune language models \n",
    "\n",
    "1. Instruction fine-tuning\n",
    "2. Classification fine-tuning\n",
    "\n",
    "<div style=\"max-width:600px\">\n",
    "    \n",
    "![](images/6.1_1.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "Instruction fine-tuning involves training a language model on a set of tasks using specific instructions to improve its ability to understand and execute tasks described in natural language prompts, as illustrated above.\n",
    "\n",
    "<div style=\"max-width:600px\">\n",
    "    \n",
    "![](images/6.1_2.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "In classification fine-tuning, the model is trained to recognize a specific set of class labels, such as \"spam\" and \"not spam\". Classification tasks can include things like identifying different species of plants from images, categorizing news articles into topics like sports, politics, and technology, and distinguishing between benign and malignant tumors in medical imaging. \n",
    "\n",
    "The key point is that a classification fine-tuned model is restricted to predicting classes it has encountered during its training. For example in the image above, the model can determine whether something is \"spam\" or \"not spam\", but cannot say anything else about the input.\n",
    "\n",
    "\n",
    "Advantages and Disadvantages of each approach:\n",
    "\n",
    "- classification fine-tuned models require less data and compute power, but its use is confined to the specific classes on which it has been trained\n",
    "- classification fine-tuned models are easier to develop\n",
    "- instruction fine-tuned models can undertake a broader range of tasks, but are harder to develop\n",
    "- instruction fine-tuning requires a larger dataset and greater computational resources to develop models proficient in various tasks\n",
    "  \n",
    "    \n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce678d49-5124-412d-bc44-9b9bf8d4caec",
   "metadata": {},
   "source": [
    "## 6.2 Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542dcf80-210b-4b1c-8afb-986589adda96",
   "metadata": {},
   "source": [
    "<h4>\n",
    "\n",
    "We will classification fine-tune the GPT model we previously implemented and pretrained. We begin by downloading a text message dataset that consists of spam and not spam messages.\n",
    "\n",
    "\n",
    "<div style=\"max-width:600px\">\n",
    "    \n",
    "![](images/6.2_1.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "The three stage process for classfication fine-tuning:\n",
    "\n",
    "1. Dataset preparation\n",
    "2. Model setup\n",
    "3. Model fine-tuning and evaluation\n",
    "\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06fc69a3-2589-4dfb-97b6-05a526e647cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sms_spam_collection/SMSSpamCollection.tsv already exists. Skipping download and extraction\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "def download_and_unzip_spam_dataset(url, zip_path, extracted_path, data_file_path):\n",
    "    if data_file_path.exists():\n",
    "        print(f\"{data_file_path} already exists. Skipping download and extraction\")\n",
    "        return\n",
    "        \n",
    "    with urllib.request.urlopen(url) as response: # downloads the file\n",
    "        with open(zip_path, \"wb\") as out_file:\n",
    "            out_file.write(response.read())\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref: # unzips file\n",
    "        zip_ref.extractall(extracted_path)\n",
    "\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "    os.rename(original_file_path, data_file_path)\n",
    "    print(f\"File downloaded and saved as {data_file_path}\")\n",
    "\n",
    "download_and_unzip_spam_dataset(url, zip_path, extracted_path, data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "205c6e89-4fd1-4d66-85ef-f131de078b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                               Text\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will ü b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeddd120-6814-47c9-8c9f-897a252bc6ac",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Lets examine the class label distribution\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2b33b01-0d90-47a4-aaa2-dde7f09c783e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a3eced-5e2a-49f7-a62d-f90fefc8b59f",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Because the data contains \"ham\" (not spam) far more than \"spam\", we will undersample the dataset to include 747 instances for each class\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8009977-849f-49cd-a05f-028b2a25361d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     747\n",
      "spam    747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def create_balanced_dataset(df):\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0] # Counts instances of spam \n",
    "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123) # randomly samples \"ham\" instances to match number of spam instances\n",
    "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]]) # combines ham subset with \"spam\"\n",
    "    return balanced_df\n",
    "\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "print(balanced_df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4422f7fe-ffa4-4a01-bf6f-de26eb22e6c3",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Now we convert the string class labels to integers:\n",
    "    \n",
    "- \"ham\" -> 0\n",
    "- \"spam\" -> 1\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb120f0d-f866-4da3-9a35-f31e8111146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\":0, \"spam\":1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2d111c-2118-4a2c-ac70-9c9df9942c3c",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Now we create a random_split function to split our dataset into train, validation, and test portions\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c04d80ed-c941-4764-aab4-50661816bc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(df, train_frac, validation_frac):\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True) # shuffles entire dataframe\n",
    "\n",
    "    # calculates split indices\n",
    "    train_end = int(len(df) * train_frac)            \n",
    "    validation_end = train_end + int(len(df) * validation_frac) \n",
    "\n",
    "    # splits df\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "\n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec1ea75d-c1dd-4b69-9d8a-92f0eac500b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df's to CSV files to be used later\n",
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a08cb4e-e474-40d5-9157-add9fe96fb7c",
   "metadata": {},
   "source": [
    "<h4>\n",
    "So far, we have downloaded the dataset, balanced it, and split it into training, validation, and testing subsets. Now we set up the PyTorch dataloaders that will be used to train the model.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fae5f2-e927-4517-a469-33ddbef4a56c",
   "metadata": {},
   "source": [
    "## 6.3 Creating data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33702c3-97b7-4ae3-b4e9-b12ebd2220bc",
   "metadata": {},
   "source": [
    "<h4>\n",
    "\n",
    "Previously, we utilized a sliding window to generate uniformly sized text chunks, which were grouped into batches for more efficient model training. Each chunk functioned as an individual training instance. \n",
    "\n",
    "However, we are now working with a dataset that contains text messages of varying lengths. To batch these messages as we did with the text chunks, we have two options:\n",
    "\n",
    "1. Truncate all messages to the shortest message in the dataset or batch\n",
    "2. Pad all messages to the length of the longest message in the dataset or batch\n",
    "\n",
    "The first option is computationally cheaper, but we can lose significant information if shorter messages are much smaller than average or longest messages, which can potentially reduce model performance. Therefore, we opt for the second option, which preserves the entire content of all messages.\n",
    "\n",
    "We can add padding tokens to all shorter messsages until their length matches that of the longest message in the dataset. We can use \"<|endoftext|>\" as a padding token.\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/6.3_1.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "However, instead of appending the string \"<|endoftext|>\" to each of the text messages directly, we can add the token ID corresponding to \"<|endoftext|>\" to the encoded messages, as shown in the figure above.\n",
    "\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa9b105d-a462-484a-93bb-2e4ab0f978a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "# Check to see what the encoding of <|endoftext|> is \n",
    "\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e94095-dc9f-4c47-8f43-ea568899d58c",
   "metadata": {},
   "source": [
    "<h4>\n",
    "\n",
    "We need to implement a Pytorch dataset which specifies how the data is loaded and processed before we can instantiate the data loaders.\n",
    "\n",
    "For this process, we instantiate the SpamDataset class which will implement the concept in the figure above. It handles several key tasks:\n",
    "\n",
    "- identifies the longest sequence in the training dataset,\n",
    "- encodes the text messages\n",
    "- ensures that all other sequences are padded with a padding token to match the length of the longest sequence\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d9eb848-e7a7-4f91-aa87-215690f0b9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbe61c7b-ce7c-4ed2-a5c1-306678dc8fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        \n",
    "        self.encoded_texts = [tokenizer.encode(text) for text in self.data[\"Text\"]] # tokenizes the texts\n",
    "\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "            self.encoded_texts = [encoded_text[:self.max_length] for encoded_text in self.encoded_texts] # truncates text sequence if longer than max_length\n",
    "\n",
    "        self.encoded_texts = [encoded_text + [pad_token_id] * (self.max_length - len(encoded_text)) for encoded_text in self.encoded_texts] # adds padding\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (torch.tensor(encoded, dtype=torch.long), torch.tensor(label, dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _longest_encoded_length(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            max_length = max(max_length, len(encoded_text))\n",
    "        return max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132f6180-8274-46dc-b65b-dc82cc22c437",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    \n",
    "This class loads the data from csv, tokenizes the text messages, allows us to pad or truncate the sequences to a uniform length determined by either the longest sequence of maximum length parameter. \n",
    "\n",
    "This ensures each tensor is of the same size, which is necessary to create batches in the training data loader we implement next\n",
    "\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b69f40c-e4e8-4ae0-b09f-f9e4c818cd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SpamDataset(\n",
    "    csv_file=\"train.csv\",\n",
    "    max_length=None,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(train_dataset.max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41df7f5d-78bc-49cf-a695-8ea0faccc70c",
   "metadata": {},
   "source": [
    "<h4>\n",
    "\n",
    "The output 120 shows that the longest text message is only 120 tokens long. The model can handle sequences up to 1024 tokens, given its context_length limit. If the dataset included texts surpassing that length, you could pass 1024 as the max_length parameter when calling this function ensuring that the data does not exceed the models supported max input length\n",
    "    \n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b17e7a6f-a7c8-4a49-8874-d193ba16336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = SpamDataset(\n",
    "    csv_file=\"validation.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file=\"test.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f828adef-1032-46c4-a0a2-b7865bd76ed7",
   "metadata": {},
   "source": [
    "<h4>\n",
    "\n",
    "Using the datasets as inputs, we can instantiate the data loaders similarly to when we were working with text data. However, in this case the input is a text sequence and the target is a class label (rather than the next token in the text). \n",
    "\n",
    "For instance, if we choose a batch size of 8, each batch will consist of eight training examples of length 120 and the corresponding class label of each example\n",
    "\n",
    "<div style=\"max-width:700px\">\n",
    "    \n",
    "![](images/6.3_2.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c228e0c-f4a5-4bde-88c5-bb707573f020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ee1f2f9-ad4e-413c-b2da-f664c9af2e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first training sample out of the eight in the batch:\n",
      " tensor([ 4805,  3824,  6158,     0,  3406,  5816, 10781, 21983,   329,   657,\n",
      "         3695, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256])\n",
      "\n",
      "Shape of inputs in whole train batch:\n",
      " torch.Size([8, 120])\n",
      "\n",
      "The first target label out of the eight in the batch:\n",
      " tensor(1)\n",
      "\n",
      "Shape of target labels in whole train batch:\n",
      " torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# for input_batch, target_batch in train_loader:\n",
    "#     pass\n",
    "# print(\"Input batch dimensions:\",input_batch.shape)\n",
    "# print(\"Target batch dimensions:\", target_batch.shape)\n",
    "\n",
    "it = iter(train_loader)\n",
    "inputs, label = next(it)\n",
    "\n",
    "print(\"The first training sample out of the eight in the batch:\\n\",inputs[0, :])\n",
    "print(\"\\nShape of inputs in whole train batch:\\n\", inputs.shape)\n",
    "print(\"\\nThe first target label out of the eight in the batch:\\n\", label[0])\n",
    "print(\"\\nShape of target labels in whole train batch:\\n\", label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f087d2-e5c9-457f-ba64-328cb6900659",
   "metadata": {},
   "source": [
    "<h4>\n",
    "\n",
    "You can see the input batch consists of eight training examples with 120 tokens each, as expected. The label tensor stores the class labels corresponding to the eight training examples.\n",
    "\n",
    "Lastly, to get an idea of the dataset size, lets print the total number of batches in each dataset.\n",
    "\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a066ae0-77fe-493a-bac9-c000a008213b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 training batches\n",
      "19 validation batches\n",
      "38 testing batches\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} testing batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5a6eae-046f-4beb-a484-3139ec153333",
   "metadata": {},
   "source": [
    "## 6.4 Initializing a model with pretrained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74ce3d0-8e8f-4c98-b205-064f3fa2924b",
   "metadata": {},
   "source": [
    "<h4>\n",
    "\n",
    "We start the process of preparing the model for classification fine-tuning by initializing our pretrained model.\n",
    "\n",
    "<div style=\"max-width:600px\">\n",
    "    \n",
    "![](images/6.4_1.png)\n",
    "\n",
    "</div>\n",
    "    \n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50ffc8d7-3633-4682-b034-40d53b7c8ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"drop_rate\": 0.0,       # Dropout rate\n",
    "    \"qkv_bias\": True        # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small-124M\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium-355M\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large-774M\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl-1558M\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-small-124M\"\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f57a6da-6b83-4a3a-ae67-3312d760cab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from previous_chapters import GPTModel, generate_text_simple, text_to_token_ids, token_ids_to_text\n",
    "\n",
    "file_name = f\"{CHOOSE_MODEL}.pth\"\n",
    "url = f\"https://huggingface.co/rasbt/gpt2-from-scratch-pytorch/resolve/main/{file_name}\"\n",
    "\n",
    "if not os.path.exists(file_name):\n",
    "    urllib.request.urlretrieve(url, file_name)\n",
    "    print(f\"Downloaded to {file_name}\")\n",
    "\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "model.load_state_dict(torch.load(file_name, weights_only=True))\n",
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea4067fd-ab16-42e8-b4b8-95efd1e0ab80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work\n"
     ]
    }
   ],
   "source": [
    "text_1 = \"Every effort moves you\"\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765928c9-4428-4720-9c78-01182f16f341",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    \n",
    "The output shows the model generates coherent text, which indicates that the model weights have been loaded correctly.\n",
    "\n",
    "Before we start fine-tuning the model as a spam classifier, lets see whether the model already classifies spam messages by prompting it with instructions\n",
    "    \n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd110d6e-6879-4af1-8a27-634d9f260115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following text 'spam'? Answer with a 'yes' or 'no':'You are a winner you have been specially selected to recieve $1000 cash or a $2000 award.'\n",
      "\n",
      "The following text 'spam'? Answer with a 'yes' or 'no':'You are a\n"
     ]
    }
   ],
   "source": [
    "text_2 = (\n",
    "    \"Is the following text 'spam'? Answer with a 'yes' or 'no':'You are a winner you have been specially selected to recieve $1000 cash or a $2000 award.'\"\n",
    ")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_2, tokenizer),\n",
    "    max_new_tokens=23,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e2619c-99dc-4ae7-b317-63f280a74a0d",
   "metadata": {},
   "source": [
    "<h4>\n",
    "\n",
    "Based on the output, it is clear the model is having trouble following instructions. This is expected, as it has only undergone pretraining and lacks instruction fine-tuning. Now, lets prepare the model for classification fine-tuning.\n",
    "  \n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ed3bd8-f846-4350-bd27-8da3c2d17ecc",
   "metadata": {},
   "source": [
    "## 6.5 Adding a classification head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c23bb1-e695-4aeb-9a16-25cf000f04a9",
   "metadata": {},
   "source": [
    "<h4>\n",
    "\n",
    "Now we must modify the pretrained LLM to prepare it for classification fine-tuning. To do this, we must replace the original output layer (that maps the last layer to a vocabulary of 50,257) to a smaller output layer that maps to two classes (0 and 1)\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/6.5_1.png)\n",
    "\n",
    "</div>\n",
    "    \n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fee02687-a017-4e61-82d7-710e790dcc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0..11): TransformerBlock × 12\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        Sequential(\n",
      "  (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (1): GELU()\n",
      "  (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      ")\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def print_gpt_model_summary(model):\n",
    "    print(\"GPTModel(\")\n",
    "    print(f\"  (tok_emb): {model.tok_emb}\")\n",
    "    print(f\"  (pos_emb): {model.pos_emb}\")\n",
    "    print(f\"  (drop_emb): {model.drop_emb}\")\n",
    "    print(f\"  (trf_blocks): Sequential(\")\n",
    "    print(f\"    (0..11): TransformerBlock × 12\")\n",
    "    print(\"      (att): MultiHeadAttention(\")\n",
    "    print(f\"        (W_query): {model.trf_blocks[0].att.W_query}\")\n",
    "    print(f\"        (W_key): {model.trf_blocks[0].att.W_key}\")\n",
    "    print(f\"        (W_value): {model.trf_blocks[0].att.W_value}\")\n",
    "    print(f\"        (out_proj): {model.trf_blocks[0].att.out_proj}\")\n",
    "    print(f\"        (dropout): {model.trf_blocks[0].att.dropout}\")\n",
    "    print(\"      )\")\n",
    "    print(\"      (ff): FeedForward(\")\n",
    "    print(f\"        {model.trf_blocks[0].ff.layers}\")\n",
    "    print(\"      )\")\n",
    "    print(f\"      (norm1): {model.trf_blocks[0].norm1}\")\n",
    "    print(f\"      (norm2): {model.trf_blocks[0].norm2}\")\n",
    "    print(f\"      (drop_resid): {model.trf_blocks[0].drop_resid}\")\n",
    "    print(\"  )\")\n",
    "    print(f\"  (final_norm): {model.final_norm}\")\n",
    "    print(f\"  (out_head): {model.out_head}\")\n",
    "    print(\")\")\n",
    "\n",
    "print_gpt_model_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feff323-f505-4456-b453-47cdcbf508d8",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    \n",
    "#### We want to replace \"out_head\" with a new output layer that we will fine-tune. \n",
    "\n",
    "#### To get the model ready for classification fine-tuning, we first freeze the model, meaning we make all layers nontrainable.\n",
    "\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e32231e5-1b88-488a-aef9-912ff4033441",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a0c2e9-2c6e-4167-b848-bcd5bca9044a",
   "metadata": {},
   "source": [
    "<h4>\n",
    "After freezing, we now replace the output layer (model.out_head) \n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7c757b5-58d0-4f17-9a4b-8085aa1dcabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(\n",
    "    in_features=BASE_CONFIG[\"emb_dim\"],\n",
    "    out_features=num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a3b092-3b48-4f49-bc3a-c24ab319068f",
   "metadata": {},
   "source": [
    "\n",
    "#### This new output layer has the attribute requires_grad set to True by default, therefore, it will be the only layer in the model that will be updated during training.\n",
    "\n",
    "#### Technically, this can be sufficient for fine-tuning, but the performance is increased when you also include the last transformer block and the final "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4224a3e8-b031-4a82-90c5-d3581d847583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
