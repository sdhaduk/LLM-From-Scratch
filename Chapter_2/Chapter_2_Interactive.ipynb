{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd0dde36-6e36-4f4d-a2d6-87749ae18df0",
   "metadata": {},
   "source": [
    "# Chapter 2 - Interactive\n",
    "\n",
    "### This notebook will contain code blocks, images, and gifs to further enhance your understanding and intuition of specific topics listed below:\n",
    "\n",
    "- #### tokenization + dataloading\n",
    "- #### positional embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65a9a60-9d2b-4d33-adca-c4745605e01e",
   "metadata": {},
   "source": [
    "## Tokenization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40af9824-b7a9-42ad-b4e0-221ae44acc4f",
   "metadata": {},
   "source": [
    "#### Lets go through the entire process of tokenizing a piece of text and creating batches of data that you can feed into your GPT2 model.\n",
    "\n",
    "#### First demonstrate what it means to turn words in a text into tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8dfcbc-e8e6-4474-9223-fd7ba729edfc",
   "metadata": {},
   "source": [
    "#### Every word or special character has a number that corresponds to it,  as you can see below, and when you tokenize a text, all you are doing is replacing a word or special character with the corresponding number.\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/interactive_1.gif)\n",
    "\n",
    "</div>\n",
    "\n",
    "#### Create a sentence using only the words below, no punctuation. Include one word that is not explicity in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6e1640f-1718-4d74-9f80-fdba886cfaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {\n",
    "    \"hello\": 1,\n",
    "    \"world\": 2,\n",
    "    \"i\": 3,\n",
    "    \"am\": 4,\n",
    "    \"learning\": 5,\n",
    "    \"tokenization\": 6,\n",
    "    \"this\": 7,\n",
    "    \"is\": 8,\n",
    "    \"fun\": 9,\n",
    "    \"<|UNK|>\": 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c53f5ebb-c678-4727-993d-9eb44fb60c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tokenization', 'is', 'so', 'fun']\n"
     ]
    }
   ],
   "source": [
    "YOUR_SENTENCE = \"tokenization is so fun\"\n",
    "\n",
    "YOUR_TEXT = YOUR_SENTENCE.split(\" \")\n",
    "print(YOUR_TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3f05d0-d4ee-4a27-a5b6-15f94f6e3708",
   "metadata": {},
   "source": [
    "#### Lets tokenize this text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0be70571-9e87-4bab-ae37-79fa769267c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 8, 10, 9]\n"
     ]
    }
   ],
   "source": [
    "YOUR_TEXT_TOKENIZED = []\n",
    "\n",
    "for word in YOUR_TEXT:\n",
    "    if word in vocabulary:\n",
    "        YOUR_TEXT_TOKENIZED.append(vocabulary[word]) # If the word is in the vocab replace it with the corresponding number\n",
    "    else:\n",
    "        YOUR_TEXT_TOKENIZED.append(vocabulary[\"<|UNK|>\"]) # If the word is not in the vocabulary replace it with 10 (corresponding number for unknown word)\n",
    "\n",
    "print(YOUR_TEXT_TOKENIZED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b41cad-24fc-4d22-8a4e-3679869580e6",
   "metadata": {},
   "source": [
    "#### Lets see how the words and numbers relate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6596059-8b82-433c-9659-f102a7d2ed11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 1 is tokenization\n",
      "In the vocabulary tokenization correlates to the number 6\n",
      "So, in the tokenized text it is 6\n",
      "\n",
      "Word 2 is is\n",
      "In the vocabulary is correlates to the number 8\n",
      "So, in the tokenized text it is 8\n",
      "\n",
      "Word 3 is so\n",
      "In the vocabulary so correlates to the number 10\n",
      "So, in the tokenized text it is 10\n",
      "\n",
      "Word 4 is fun\n",
      "In the vocabulary fun correlates to the number 9\n",
      "So, in the tokenized text it is 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, token_id in enumerate(YOUR_TEXT_TOKENIZED):\n",
    "    print(f\"Word {index+1} is {YOUR_TEXT[index]}\\nIn the vocabulary {YOUR_TEXT[index]} correlates to the number {token_id}\\nSo, in the tokenized text it is {token_id}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e797055-7865-434d-8f71-2e84c7082be0",
   "metadata": {},
   "source": [
    "## Dataloading:\n",
    "\n",
    "#### Remember, LLMs are pretrained by trying to predict the next word in a sequence of words.\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/interactive_2.gif)\n",
    "\n",
    "</div>\n",
    "\n",
    "#### Lets implement this in code. First, lets see your sentence and the tokenized version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0b9a3b6-7a97-4637-ac65-b60e4129c1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tokenization', 'is', 'so', 'fun']\n",
      "\n",
      " [6, 8, 10, 9]\n"
     ]
    }
   ],
   "source": [
    "print(YOUR_TEXT)\n",
    "print(\"\\n\",YOUR_TEXT_TOKENIZED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3f3d6e-7ccc-47cd-ae5a-f5ec8cbedfc8",
   "metadata": {},
   "source": [
    "#### Next lets create the input and target pairs. We want the first element in the inputs array and target array to correspond to eachother. Meaning, the first element in the targets array is the word we want the LLM to predict is next when fed the first word in the sequenec. The second element in the targets array should be the word the LLM predicts when fed the second word in the sequence, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78a179fb-e35b-4ee4-8751-9a1bc3c1a7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "for i in range(len(YOUR_TEXT) - 1):\n",
    "    inputs.append(YOUR_TEXT[i])\n",
    "    targets.append(YOUR_TEXT[i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed9777d-9097-4ea4-beca-614be46c84bf",
   "metadata": {},
   "source": [
    "#### Lets analyze what this code does. We have our forloop which will iterate through every word in the sequence, but we have the loop end before we get to the last word in the sequence. Why? \n",
    "\n",
    "#### Because, the LLM is supposed to predict the next word in the sequence, given a word. If we give it the last word in the sequence, there is nothing left to predict. Therefore, we stop once we get to the second to last word.\n",
    "\n",
    "#### Lets see what is in our inputs and targets arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e2eaaba-e445-437b-89b1-50c5c5fa6dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input words: ['tokenization', 'is', 'so']\n",
      "Target words: ['is', 'so', 'fun']\n"
     ]
    }
   ],
   "source": [
    "print(\"Input words:\", inputs)\n",
    "print(\"Target words:\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a87373c-11a4-4f75-b574-1a5551ea6f6b",
   "metadata": {},
   "source": [
    "#### As you can see, its just like how I described above. \n",
    "\n",
    "#### The first element in inputs is the first word in the sequence. The first element in targets is the second word in the sequence.\n",
    "#### The second element"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
