{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfbf4986-6a28-486c-a799-6105b7ded7a6",
   "metadata": {},
   "source": [
    "# Chapter 2: Working with text data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f93b2cc-6c0f-4669-add0-ef4c97d78a1f",
   "metadata": {},
   "source": [
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/2.0.png)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bca7cb7-02ae-4e64-bfe1-1c3ff81fbe34",
   "metadata": {},
   "source": [
    "## 2.1 Understand word embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b06badf-0ce1-4821-b13d-52b735291fa6",
   "metadata": {},
   "source": [
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/2.1.png)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c34766-6ef6-489b-931b-33cec2327bec",
   "metadata": {},
   "source": [
    "## 2.2 Tokenizing Text\n",
    "* Splitting the input text into individual tokens, a required step for creating embeddings for an LLM\n",
    "* These tokens can be individual words or special characters, including punctuation characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a062714-8588-4f32-8b2f-b6130d7e8000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the-verdict.txt', <http.client.HTTPMessage at 0x1077967e0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "file_path = \"the-verdict.txt\"\n",
    "\n",
    "urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5582a0b-f4ec-42e5-b6ad-bf7807f0e797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f10eeb65-5c79-438c-b37d-f0aafa2a750d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45495e88-df6b-4ebf-889f-fbf31807acf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5eccb288-46e2-4df4-a932-046b757c91ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d4cedcb-b6d6-4b5e-a91e-7e58f152ae65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd510b4d-7e6a-4a42-8840-e3e4f839151e",
   "metadata": {},
   "source": [
    "## 2.3 Converting tokens into token IDs\n",
    "* #### Convert hese tokens from strings into an integer representation to produce token IDs - intermediate step before converting token IDs into embedding vectors\n",
    "\n",
    "#### To get token IDs:\n",
    "#### 1. Build a vocabulary from these previously generated tokens - Each unique token is added to the vocabulary in alphabetical order\n",
    "\n",
    "  \n",
    "#### 2. Each unique token is mapped to a unique integer called token ID\n",
    "\n",
    "\n",
    "#### For Example:\n",
    "\n",
    "#### Apple -> 0\n",
    "\n",
    "#### Axle -> 1\n",
    "\n",
    "#### .\n",
    "\n",
    "#### .\n",
    "\n",
    "#### .\n",
    "\n",
    "#### Zebra -> 10,000\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2abc82f3-f0c7-47f0-9d60-584d29eecbb0",
   "metadata": {},
   "source": [
    "<div style=\"max-width:1000px\">\n",
    "    \n",
    "![](images/2.3.png)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1d328a9-2bec-4b05-8ef9-e14ae6b74968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8583a45-c065-4723-afd6-2743deee8ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "vocab =  {token:index for index, token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24ea5048-3710-4b24-8d5a-7767377b1ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8ab0f3f-bf01-4673-9c4b-afc41f67de35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "565c4533-c01d-4785-ba3f-093cbca3fafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "752b33b1-c51c-461b-a630-def8b56d44fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, do you like tea?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mencode(text))\n",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m, in \u001b[0;36mSimpleTokenizerV1.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[1;32m      8\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[0;32m----> 9\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d86d83d-b7e0-4c8b-9361-48d81589497b",
   "metadata": {},
   "source": [
    "## 2.4 Adding special context tokens\n",
    "* #### add \"< unk >\" token to vocab when tokenizer comes across a word that is not in the vocabulary\n",
    "* #### add \"< endoftext >\" token to vocab to signify a document boundary. When training LLM on independent documents, it is common to insert this token before each document to signfy to the LLM that these text sources are unrelated (because they will be concatenated when training)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2eb1bb99-40e9-4234-ae18-8c34baf0b1a4",
   "metadata": {},
   "source": [
    "![](images/2.4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63b8a1aa-2bb8-4205-9a8d-a8b25dd3cb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42d30497-553b-4b85-8675-b9648849bbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69fefef5-bd59-4ad5-86e7-ae86bd0b3657",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9972443-5134-4373-b181-6ac5d87ca741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bf634af-ee47-4335-a85f-af9cad915012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1def10e5-e3c7-41c6-878d-bcc4143bac97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa74ab1-5221-4b52-81b2-06f713b98e21",
   "metadata": {},
   "source": [
    "## 2.5 Byte pair encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d81359f-b0a4-4dda-976c-e2b9a8219e1f",
   "metadata": {},
   "source": [
    "\n",
    "#### BPE tokenizer was used to train GPT-2, GPT-3 and the original model used in ChatGPT\n",
    "\n",
    "#### Doesn't use <|unk|> token - then how does it deal with unknown words?\n",
    "\n",
    "#### The algorithm underlying BPE breaks down words that aren't in its predefined vocab into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words. So, if BPE encounters an unknown word, it can represent it as a sequence of subword tokens or characters\n",
    "\n",
    "<div style=\"max-width:1000px\">\n",
    "    \n",
    "![](images/2.5_1.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "#### The ability to break down unknown words into individual characters ensures that the tokenizer and, consequently, the LLM that is trained with it can process any text, even if it contains words that were not present in its training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "194eba0e-d009-4315-adb2-4efeac294f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91fee70e-d1f6-4102-b980-d54e0573250c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 20035, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someUnknownPlace.\"\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d382dbf4-1d7c-4f02-8c53-fe702ea0b381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someUnknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122ddeca-bb32-4e1d-8861-d680cf307e3e",
   "metadata": {},
   "source": [
    "## 2.6 Data sampling with a sliding window\n",
    "\n",
    "#### Introduce Dataloaders and Dataset class from PyTorch\n",
    "* #### Dataset -> instantiates an object that decides how a record in the dataset will be structured\n",
    "* #### Dataloader -> decides how the data is shuffled and assembled into batches\n",
    "\n",
    "#### batch_size -> how many records in a batch\n",
    "\n",
    "#### max_length -> how many words in a record\n",
    "\n",
    "#### stride -> how many indices to slide forward (imagine a sliding window)\n",
    "\n",
    "#### drop_last -> drops the last batch if it is shorter than the specified batch size\n",
    "\n",
    "#### num_workers -> number of CPU processes used for preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e71f9d9-a4c9-4afe-a65b-a12ec405a6f2",
   "metadata": {},
   "source": [
    "<h4>\n",
    "The next step is to generate input-target pairs for training the LLM.\n",
    "</h4>\n",
    "\n",
    "![](images/2.6_1.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a670be34-e044-4234-80fa-e5f28f36f1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e4b2eaa-ef4a-424d-b932-5a88a8a1a6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a01cfc4-9a63-46d4-9c3b-e5f285481bf4",
   "metadata": {},
   "source": [
    "#### Generating the input-target pairs -> LLMS are pretrained by trying to predict the next word in a text. \n",
    "\n",
    "#### Create two variables, x and y, where x contains the input tokens, and y cointains the targets (which are the inputs shifted by +1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "525539d6-5bbb-4b1c-9365-bfb746856780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1: context_size + 1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97c944c0-1898-45d2-aecd-5ba7cac6cfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"---->\", desired)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9e886a9-ddb1-4c3a-965b-67222a62ebee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceac7149-e967-4252-96bf-182be82b2d19",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### We've now created the input-target pairs we can use for LLM training.\n",
    "\n",
    "#### The last thing we must do before we turn these tokens into embeddings is implement an efficient data loader that iterates over the input dataset and returns the inputs and targets as PyTorch tensors. We want to tensors in particular: the input tensor, containing the text that the LLM sees and the target tensor, containing the targets for the LLM to predict.\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/2.6_2.png)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7044a5d6-738f-4109-9a90-6baad1fd7026",
   "metadata": {},
   "source": [
    "## ---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c7bccc-8260-4ff5-b46b-26dbbf4abb92",
   "metadata": {},
   "source": [
    "#### Explaining Dataset and DataLoader classes from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e43bf35-5b82-4233-bfb1-ce71c18d5f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5605, -0.4598],\n",
      "        [-0.8939, -2.0963],\n",
      "        [ 0.1645,  0.2967],\n",
      "        [ 0.7719, -0.1005],\n",
      "        [ 2.6913,  0.8113]])\n",
      "tensor([0, 0, 0, 1, 1])\n",
      "tensor([[ 0.3835,  1.2684],\n",
      "        [-1.9615,  1.1403]])\n",
      "tensor([0, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "X_train = torch.randn(5, 2)\n",
    "y_train = torch.tensor([0, 0, 0, 1, 1])\n",
    "X_test = torch.randn(2, 2)\n",
    "y_test = torch.tensor([0, 1])\n",
    "\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "print(X_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b03a8d83-1771-4d22-a91b-827b4342b94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class instantiates objects that define how each data record is loaded\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ToyDataset(Dataset):            \n",
    "    def __init__(self, X, y):        # set up attributes that we can access later in the __getitem__ and __len__ methods (filepaths, file objects, database connectors, etc)\n",
    "        self.features = X\n",
    "        self.labels = y\n",
    "\n",
    "    def __getitem__(self, index):    # define instructions for returning exactly one item from the datset via an index \n",
    "        one_x = self.features[index]\n",
    "        one_y = self.labels[index]\n",
    "        return one_x, one_y\n",
    "        \n",
    "    def __len__(self):               # contains instructions for retrieving the length of the dataset \n",
    "        return self.labels.shape[0]\n",
    "\n",
    "train_ds = ToyDataset(X_train, y_train)\n",
    "test_ds = ToyDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c9fb104-86ce-46fa-a4f8-dfb3fce37a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader class handles how the data is shuffled and assembled into batches\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b81d23a5-29df-4019-87c4-60baf8021d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: tensor([[-0.8939, -2.0963],\n",
      "        [ 0.1645,  0.2967]]) tensor([0, 0])\n",
      "Batch 2: tensor([[ 2.6913,  0.8113],\n",
      "        [ 0.7719, -0.1005]]) tensor([1, 1])\n"
     ]
    }
   ],
   "source": [
    "for idx, (x, y) in enumerate(train_loader):\n",
    "    print(f\"Batch {idx + 1}:\", x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56466600-53e4-401d-b9f0-b668e7167616",
   "metadata": {},
   "source": [
    "## ---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c0b8f80-55a8-4cac-a5dd-0742cd51c506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "        for i in range(0, len(token_ids) - max_length, stride): # terminate forloop at [ len(token_ids) - max_length ] in order to guarantee input_chunk and target_chunk are of the same size\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5457e045-e79d-486e-bb2d-59d534d78302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e33b1ae1-c23e-432f-91e4-a79b8721f110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e92158bc-cfd0-4c9c-af5d-d5160ca494df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9b67b3-b840-46e9-9b83-72dd21eb5259",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### The first batch contains two tensors: the firt tensor stores the input token IDs, and the second tensor stores the target token IDs. Since the max_length is set to 4, each of the two tensors contain four token IDs. \n",
    "\n",
    "#### To understand the meaning of stride=1, take a look at the second batch. If we compare the first and second batches we notice that the second batch's token IDs are shifted by one position (for example the second ID in the first batch's input is 367, which is the first ID of the second batch's input). The stride setting dictates the number of positions the inputs shift across batches.\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/2.6_3.png)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672f30a8-bbaf-4164-b6c5-11fb0eb79283",
   "metadata": {},
   "source": [
    "## 2.7 Creating token embeddings\n",
    "\n",
    "#### The last step in preparing the input text for LLM training is to convert the token IDs into embedding vectors, as shown below.\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/2.7_1.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "#### We initialize these embedding weights with random values. This serves as the starting point for the LLM's learning process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3efe58-75ef-4023-800c-79670a7fc803",
   "metadata": {},
   "source": [
    "#### Lets demonstrate how the token ID to embedding vector conversion works\n",
    "\n",
    "#### Suppose we have the following four input tokens with IDs 2, 3, 5, and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c0f361cf-fc6d-4370-8f6a-f16befdc524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1edd98-7226-44e0-bc70-3ffa43d4176a",
   "metadata": {},
   "source": [
    "#### Suppose we have a small vocabulary of only 6 words (instead of 50,257 words like in BPE), and we want to create embeddings of size 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "96624c9a-ae63-49c7-8218-4de759d207bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66c499b-866c-4d72-bf70-a8ac86b31504",
   "metadata": {},
   "source": [
    "#### Using vocab_size and output_dim, we can instantiate an embedding layer in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "93bc993e-3a75-4644-b4c0-a23836092a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.9416, -0.6812, -0.4983],\n",
      "        [-0.2112,  0.3875,  0.3281],\n",
      "        [-0.5844, -0.4397, -0.5584],\n",
      "        [ 0.0242, -0.4170, -0.3838],\n",
      "        [ 1.6351, -1.0261,  0.1873],\n",
      "        [ 0.5751,  0.5681,  1.7211]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee3a8b9-b7b9-4e17-b3ed-63cc83474c92",
   "metadata": {},
   "source": [
    "#### The weight matrix of the embedding layer contains small, random values. These values are optimized during LLM training.\n",
    "\n",
    "#### Moreover, we can see the weight matrix has six rows and three columns. These is one row for each of the six possible tokens in the vocabulary, and there is one column for each of the three embedding dimensions.\n",
    "\n",
    "#### Now, lets apply it to a token ID to obtain the embedding vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d77ef017-007f-44c1-b108-c82cddbcc528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0242, -0.4170, -0.3838]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a799b0d1-b130-4c95-a29e-e6e1a044df7b",
   "metadata": {},
   "source": [
    "#### You can see that for token ID 3, the third row in the embedding layer is returned as this tokens embedding. In other words, the embedding layer is essentially a lookup operation that retrieves rows from the embedding layer's weight matrix via token ID.\n",
    "\n",
    "#### Lets convert all four input IDs to embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d84210c-8d7e-4dc2-b6c2-803c1cfbe929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5844, -0.4397, -0.5584],\n",
      "        [ 0.0242, -0.4170, -0.3838],\n",
      "        [ 0.5751,  0.5681,  1.7211],\n",
      "        [-0.2112,  0.3875,  0.3281]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713bb1d0-a613-4d59-818b-07825d03819b",
   "metadata": {},
   "source": [
    "#### Each row in this output matrix is obtained via a lookup operation from the embedding weight matrix, as shown below\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/2.7_2.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "#### Next we add a small modification to these embedding vectors to encode positional information about a token within a text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fdb2cc-aeee-462a-ac14-c007a7d99fb3",
   "metadata": {},
   "source": [
    "## 2.8 Encoding word positions\n",
    "\n",
    "#### The self-attention mechanism doesn't have a notion of position or order for the tokens within a sequence, so it is helpful to inject additional position information into the LLM.\n",
    "\n",
    "#### To achieve this we can use either of two broad categories of position-aware embeddings: relative positional embeddings and absolute positional embeddings. \n",
    "\n",
    "#### Absolute positional embeddings are directly associated with specific positions in a sequence. For each poisition in the input sequence, a unique embedding will be added to the tokens embedding to convey location, as shown below.\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/2.8_1.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "#### Relative positional embeddings focus on the relative position or distance between tokens. This means the model learns the relationship in terms of \"how far apart\", rather than \"at which exact poisition.\" The advantage is that the model can generalize better to sequences of varying lengths. OpenAI's GPT models use absolute positional embeddings that are optimized during the training process rather than being fixed or predefined like the positional encodings in the original transformer model. \n",
    "\n",
    "#### Lets create initial positional embeddings to create LLM inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "380da253-1861-4fdf-83a4-ff8c5e55b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input tokens into 256-dimensional vector represenations\n",
    "\n",
    "vocab_size = 50267 # vocab size of BPE tokenizer\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7261bba1-2e8b-4879-9944-770e545e9a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ids:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Input shape: \n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token ids:\\n\", inputs)\n",
    "print(\"\\nInput shape: \\n\", inputs.shape) # data branch consists of 8 samples with 4 tokens each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6db45826-55dd-4a81-bd3a-21bc0921a3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape) # each token is now embedding as a 256-dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39e5cfaf-8f73-48e1-a233-e201c5811cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "# For GPT's absoulute embedding approach, we need to create another embedding layer that has the same embedding dimension as the \"token_embedding_layer\"\n",
    "\n",
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length)) # input to pos_embeddings is usually a placeholder vector that contains sequence of numbers 0, 1, 2 ... input_length - 1\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b27d9be-9316-4a36-a1cf-d2519b5bf8b5",
   "metadata": {},
   "source": [
    "#### The input to pos_embeddings is usually a placeholder vector torch.arange(context_length), which contains sequence of numbers 0, 1, ..., up to the maximum input length-1. A\n",
    "\n",
    "#### As you can see, the pos_embeddintg tensor contains four 256-dimensional vecotrs. We can now add these directly to the token embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3cc00bf7-656f-43bd-ac3b-3b1f39c722e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5ce500-cf7c-4ed5-9ac1-26011fb649f7",
   "metadata": {},
   "source": [
    "#### These input_embeddings can now be processed by"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
