{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06edaad2-1a1c-45fc-b5b1-f237a8e07a25",
   "metadata": {},
   "source": [
    "# Chapter 4 - Interactive\n",
    "\n",
    "### This notebook will contain code blocks, images, and gifs to further enhance your understanding and intuition of specific topics listed below:\n",
    "\n",
    "- #### Transformers\n",
    "- #### GPTModel Outhead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c1a345-a9fb-4b29-9ffd-ac9c59c62b00",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801b695f-397e-4d44-949c-a2612dd35729",
   "metadata": {},
   "source": [
    "#### The image below is from this website that has a 3D-visual of the GPT architecture: https://bbycroft.net/llm\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/interactive_5.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "#### The images below make up the transformer block in the GPT architecture.\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/interactive_4.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/interactive_3.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "#### Lets explain each part:\n",
    "1. #### LayerNorm 1 - This is the first operation applied to the input from the previous layer (or the embedding layer for the first block). It standardizes each token’s embedding — so that the values across features have zero mean and unit variance.\n",
    "2. #### Q, K, V Projections - We take the normalized token embeddings and run them through three different linear layers to create: Q (Query), K (Key), V (Value).\n",
    "3. #### Attention Scores (Scaled Dot Product) - Compute scaled-dot product between the Query and Key matrices, then apply softmax to make them probabilities.\n",
    "4. #### Apply Attention Weights to V - The attention weights are multiplied by the V vectors to get a new, context-aware representation for each token.\n",
    "5. #### Concatenate Heads - All heads are concatenated and passed through another linear layer (the “output projection”).\n",
    "6. #### Residual Connections - Adds the original input (pre-attention) back to the attention output. This helps preserve the original signal and improves gradient flow. Essential for stable deep learning models.\n",
    "7. #### LayerNorm 2 - Same as before — normalize the updated representation token-wise across features. Prepares the token embeddings for the next transformation (MLP) in a more numerically stable way.\n",
    "8. #### MLP FeedForward Layer - First linear layer expands the embedding dimension (e.g., 768 → 3072). Activation function (usually GELU or ReLU) introduces non-linearity. Second linear layer contracts it back to the original dimension.\n",
    "9. #### Final Residual Connection - Adds the original input to the MLP back into the output of the MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323f91d4-d69b-459f-a581-adc75d118f56",
   "metadata": {},
   "source": [
    "#### Now lets implement our own mini-version to enhance your understanding:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1282e886-4035-462e-83f9-29adcaba197a",
   "metadata": {},
   "source": [
    "#### Step 1: Setup and Fake Input Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a44f45ce-e36b-437c-aae7-e126821f1467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Embeddings:\n",
      " tensor([[ 1.9269,  1.4873, -0.4974,  0.4396, -0.7581,  1.0783],\n",
      "        [ 0.8008,  1.6806,  0.3559, -0.6866,  0.6105,  1.3347],\n",
      "        [-0.2316,  0.0418, -0.2516,  0.8599, -0.3097, -0.3957]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Toy settings\n",
    "seq_len = 3\n",
    "emb_dim = 6\n",
    "\n",
    "# Simulate embeddings for a 3-token sequence\n",
    "x = torch.randn(seq_len, emb_dim)\n",
    "print(\"Input Embeddings:\\n\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d767ccf-6fcb-423d-bb09-79cd2f62624b",
   "metadata": {},
   "source": [
    "#### We begin with a fake sequence of 3 token embeddings, each of size 6. This simulates the output from the previous transformer block or the embedding layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464b3ee1-ef02-4a34-813f-5428706f71d1",
   "metadata": {},
   "source": [
    "#### Step 2: Firsy LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ee1b166-e05e-4419-9297-70e76422e632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After First LayerNorm:\n",
      " tensor([[ 1.3309,  0.8856, -1.1243, -0.1754, -1.3883,  0.4715],\n",
      "        [ 0.1565,  1.3214, -0.4327, -1.8131, -0.0956,  0.8635],\n",
      "        [-0.4298,  0.2095, -0.4765,  2.1229, -0.6125, -0.8136]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# First LayerNorm\n",
    "layer_norm1 = nn.LayerNorm(emb_dim)\n",
    "x_norm = layer_norm1(x)\n",
    "print(\"After First LayerNorm:\\n\", x_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddac82b7-9181-4ec3-a996-bb199b2efa37",
   "metadata": {},
   "source": [
    "#### We normalize each token’s features to have zero mean and unit variance. This stabilizes the inputs to the attention mechanism.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057ebe39-fe7a-47ac-afcc-51ec953246aa",
   "metadata": {},
   "source": [
    "#### Step 3: Q, K, V Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb84ae1d-440d-4f2b-909f-d89bf18c3f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Vectors:\n",
      " tensor([[-0.2050, -0.6209,  0.6756, -0.4645, -0.0528, -0.0239],\n",
      "        [-0.4284, -1.0410,  0.3973,  0.0177,  0.0273,  0.0124],\n",
      "        [ 0.8572,  0.6222, -0.3372,  0.4344, -0.7646, -0.3994]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "Key Vectors:\n",
      " tensor([[ 0.6382, -0.2814, -0.5932, -0.4497, -0.9684, -0.2530],\n",
      "        [ 0.4410, -0.7229, -0.4119,  0.5000, -0.5736,  0.4171],\n",
      "        [-0.5239,  1.1588, -0.3364, -0.5223,  0.0211, -0.2605]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "Value Vectors:\n",
      " tensor([[-0.7430, -0.1025, -0.5163, -0.8837,  1.0014, -1.3047],\n",
      "        [ 0.3022, -0.4374, -0.3117,  0.1834, -0.2640, -0.7827],\n",
      "        [-0.5925, -0.0414, -0.0562, -0.6907,  0.8363, -0.0091]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Linear projections for Q, K, V (no bias for clarity)\n",
    "W_q = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "W_k = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "W_v = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "\n",
    "Q = W_q(x_norm)\n",
    "K = W_k(x_norm)\n",
    "V = W_v(x_norm)\n",
    "\n",
    "print(\"Query Vectors:\\n\", Q)\n",
    "print(\"Key Vectors:\\n\", K)\n",
    "print(\"Value Vectors:\\n\", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fca4ae8-a05e-4b9d-a064-8cf0917e76c9",
   "metadata": {},
   "source": [
    "#### We create linear transformations that project the input into query, key, and value spaces. These are used to compute attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd2ae09-c338-47f3-b47c-77b7f02ac56b",
   "metadata": {},
   "source": [
    "#### Step 4: Compute Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d97c16ac-b563-4a70-b76a-4e9215711f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Scores:\n",
      " tensor([[-0.0371, -0.0538, -0.2415],\n",
      "        [-0.1036,  0.1626, -0.4603],\n",
      "        [ 0.4973,  0.2271,  0.1006]], grad_fn=<DivBackward0>)\n",
      "Attention Weights (Softmaxed):\n",
      " tensor([[0.3573, 0.3514, 0.2913],\n",
      "        [0.3328, 0.4343, 0.2329],\n",
      "        [0.4105, 0.3133, 0.2761]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute attention scores and softmax\n",
    "d_k = emb_dim\n",
    "scores = Q @ K.T / d_k**0.5\n",
    "attn_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "print(\"Attention Scores:\\n\", scores)\n",
    "print(\"Attention Weights (Softmaxed):\\n\", attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acdfe81-332b-4420-8894-4ec5ac2045f0",
   "metadata": {},
   "source": [
    "#### This computes the compatibility between each query and key (dot product), scales it, and converts it to a probability distribution using softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7cfd02-1a9d-44e3-a2e3-5f3804112079",
   "metadata": {},
   "source": [
    "#### Step 5: Compute Attention Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42407ca0-d550-455d-9cc2-b04766ff3ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output:\n",
      " tensor([[-0.3319, -0.2024, -0.3104, -0.4525,  0.5086, -0.7439],\n",
      "        [-0.2540, -0.2337, -0.3203, -0.3753,  0.4134, -0.7762],\n",
      "        [-0.3739, -0.1906, -0.3252, -0.4960,  0.5593, -0.7834]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Attention output is weighted sum of value vectors\n",
    "attn_output = attn_weights @ V\n",
    "print(\"Attention Output:\\n\", attn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6010e361-8c3b-47d4-860b-ed57158ed278",
   "metadata": {},
   "source": [
    "#### This combines value vectors according to attention weights — giving us contextualized token embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d6a9df-42ba-4da4-9003-6169e1b2e407",
   "metadata": {},
   "source": [
    "#### Step 6: Attention Output Projection + Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65cf78ea-3fd9-4094-97e5-3d8e8205879f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-Attention Residual Output:\n",
      " tensor([[ 1.7323,  1.6815, -0.4611,  0.5775, -0.7837,  0.8959],\n",
      "        [ 0.6157,  1.9135,  0.4339, -0.5653,  0.5614,  1.1892],\n",
      "        [-0.4475,  0.2324, -0.2285,  1.0092, -0.3245, -0.6006]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Final linear projection back to emb_dim\n",
    "proj = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "projected = proj(attn_output)\n",
    "\n",
    "# Add residual connection (original input + projected attention output)\n",
    "x_resid_attn = x + projected\n",
    "print(\"Post-Attention Residual Output:\\n\", x_resid_attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe738ef-261a-471b-8eb1-f8e7a3077fa2",
   "metadata": {},
   "source": [
    "#### We project the multi-head result back to embedding dimension and add the residual connection to preserve the original input signal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880af9d1-ba0b-4913-ab79-0b1172a73df4",
   "metadata": {},
   "source": [
    "#### Step 7: Second LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26babe32-7ea7-4597-9fe5-3313f6071499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Second LayerNorm:\n",
      " tensor([[ 1.1668,  1.1141, -1.1077, -0.0307, -1.4421,  0.2995],\n",
      "        [-0.1004,  1.6212, -0.3416, -1.6670, -0.1725,  0.6604],\n",
      "        [-0.7138,  0.5383, -0.3105,  1.9686, -0.4871, -0.9956]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Second LayerNorm before MLP\n",
    "layer_norm2 = nn.LayerNorm(emb_dim)\n",
    "x_norm2 = layer_norm2(x_resid_attn)\n",
    "print(\"After Second LayerNorm:\\n\", x_norm2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5b3b07-209f-420e-a341-53580c775d3b",
   "metadata": {},
   "source": [
    "#### Normalizes again before feeding into the feedforward MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3094be90-3c8a-4242-b6ec-f3d4882c3f5c",
   "metadata": {},
   "source": [
    "#### Step 8: MLP Feedforward (2-layer NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a94ef752-a18b-45c8-b1af-bef6193b2527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Intermediate Projection Shape: torch.Size([3, 12])\n",
      "MLP Activation Shape: torch.Size([3, 12])\n",
      "MLP Output Shape: torch.Size([3, 6])\n",
      "MLP Output:\n",
      " tensor([[ 0.1352,  0.5397, -0.1804, -0.3725, -0.2662,  0.3049],\n",
      "        [-0.0028,  0.2650, -0.2442, -0.3500, -0.0057,  0.0221],\n",
      "        [ 0.2010,  0.5542, -0.0727, -0.0812, -0.0789, -0.0450]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# MLP: Linear → GELU → Linear\n",
    "mlp_hidden = 12  # Hidden size for demonstration\n",
    "linear1 = nn.Linear(emb_dim, mlp_hidden)\n",
    "linear2 = nn.Linear(mlp_hidden, emb_dim)\n",
    "\n",
    "# Apply MLP\n",
    "mlp_intermediate = linear1(x_norm2)\n",
    "print(\"MLP Intermediate Projection Shape:\", mlp_intermediate.shape)\n",
    "\n",
    "mlp_activated = F.gelu(mlp_intermediate)\n",
    "print(\"MLP Activation Shape:\", mlp_activated.shape)\n",
    "\n",
    "mlp_output = linear2(mlp_activated)\n",
    "print(\"MLP Output Shape:\", mlp_output.shape)\n",
    "print(\"MLP Output:\\n\", mlp_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63d04af-870a-44ab-a738-356cf4283802",
   "metadata": {},
   "source": [
    "#### Applies a small feedforward neural network to each token independently. First expands, then contracts the representation, with a GELU non-linearity in the middle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a8544a-697d-46c7-a566-0dfd8ec3932f",
   "metadata": {},
   "source": [
    "#### Step 9: Final Residual Connection (Post-MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f95b16a9-27c2-4b4a-af54-fba75e3d36ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Output of Transformer Block:\n",
      " tensor([[ 1.8675,  2.2212, -0.6416,  0.2050, -1.0499,  1.2008],\n",
      "        [ 0.6129,  2.1785,  0.1896, -0.9153,  0.5557,  1.2112],\n",
      "        [-0.2465,  0.7866, -0.3012,  0.9280, -0.4034, -0.6456]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Final residual connection\n",
    "final_output = x_resid_attn + mlp_output\n",
    "print(\"Final Output of Transformer Block:\\n\", final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b672cca5-58b0-43d2-a5c9-e1b9b24ce5ac",
   "metadata": {},
   "source": [
    "#### Adds the result of the MLP back to the post-attention representation. This is the final output of the transformer block — ready to be passed to the next block or output head."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c46d126-6a09-40a6-843d-582b43525e2d",
   "metadata": {},
   "source": [
    "## GPTModel Outhead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb6563c-53c0-479a-985c-f7be1a4d00e8",
   "metadata": {},
   "source": [
    "#### The image below is from this website that has a 3D-visual of the GPT architecture: https://bbycroft.net/llm\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/interactive_2.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "#### The image below is the architecture of the block above containing LayerNorm -> Linear -> Softmax highlighted in blue.\n",
    "\n",
    "<div style=\"max-width:800px\">\n",
    "    \n",
    "![](images/interactive_1.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "#### Lets explain each part:\n",
    "\n",
    "1. #### LayerNorm - The output tensor from the last transformer block goes undergoes this final Layer Normalization. The LN Agg: <b>μ</b>,<b>σ</b> show that LayerNorm computes the mean (μ) and standard deviation (σ) across the features. Finally, it normalizes each token embedding using learned parameters: $ $<b>γ</b> and <b>β</b> (gamma and beta) are learnable scale and shift parameters.\n",
    "\n",
    "3. #### Linear Layer (LM Head) - The normalized output is passed to a linear projection layer, referred to here as LM Head Weights. It transforms the final hidden state into logits, one for each token in the vocabulary.\n",
    "\n",
    "4. #### Logits Calculation - Each token’s embedding now corresponds to a logit vector of shape [vocab_size]. These logits represent the unnormalized scores for each token being the next in the sequence.\n",
    "\n",
    "5. #### SM Agg (Softmax Aggregation) - The logits are passed through a Softmax layer, converting the logits to a probability distribution over the vocabulary.\n",
    "\n",
    "6. #### Logits Softmax Output - The output is the predicted probability distribution for the next token. In the visual: Tokens like C, B, A, etc. are shown with their associated probabilities. The model seems to have assigned the highest probability to A, meaning it predicts that A is the most likely next token.\n",
    "\n",
    "#### Now lets implement our own mini version to enhance your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e537fb2-0bfc-47cf-8931-b7c21f985bf0",
   "metadata": {},
   "source": [
    "#### Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea38ecbc-dd40-4075-a47e-314f7d26e2bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10cfeb650>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c0cca2-04d1-40c4-9141-964a7c80ee19",
   "metadata": {},
   "source": [
    "#### We start by importing PyTorch and setting a manual seed to ensure consistent results each time we run the notebook. This will help us get reproducible random embeddings and model weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b5f04-20b6-4bf9-95a4-fb1bcd0b21fe",
   "metadata": {},
   "source": [
    "#### Step 2: Define Fake Token Embedding (output of the last Transformer block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c1781b1-6fc0-49be-b7a6-8491c1779ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final embedding: tensor([ 0.3367,  0.1288,  0.2345,  0.2303, -1.1229, -0.1863])\n"
     ]
    }
   ],
   "source": [
    "# Simulate final token embedding of dimension 6\n",
    "emb_dim = 6\n",
    "final_embedding = torch.randn(emb_dim)\n",
    "print(\"Final embedding:\", final_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc15df0-22e6-4a77-9cdb-bf94a0658c70",
   "metadata": {},
   "source": [
    "#### Here we simulate the output from the last transformer block for a single token. This is just a random vector of length 6 (our simulated embedding dimension), which represents the information learned about this token up to this point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01500342-150a-4e02-bd71-3f4ddc7bbaa9",
   "metadata": {},
   "source": [
    "#### Step 3: Apply LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bbc3a81-f30b-4171-8986-22700f579e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After LayerNorm: tensor([ 0.7971,  0.3827,  0.5933,  0.5851, -2.1126, -0.2456],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# LayerNorm applied to a single token's embedding\n",
    "layer_norm = nn.LayerNorm(emb_dim)\n",
    "normalized_embedding = layer_norm(final_embedding)\n",
    "print(\"After LayerNorm:\", normalized_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e7d421-fba9-4b9c-b616-fd063d13ac4d",
   "metadata": {},
   "source": [
    "#### LayerNorm normalizes the embedding so that it has zero mean and unit variance (per token). It also learns a scale (γ) and shift (β) for each dimension during training. This helps stabilize training and improves convergence in transformer models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b932b51f-10a8-40ea-bf24-cc64d19aa2cb",
   "metadata": {},
   "source": [
    "#### Step 4: Project Embedding to Vocabulary Logits (LM Head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4ea5e28-a09d-418d-9e62-e76117d0977f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([-0.3350,  0.0321, -0.5233,  0.6060, -0.0765, -0.2038,  0.3173, -0.3729,\n",
      "        -0.3371, -0.9188], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "# Define vocab size and linear projection\n",
    "vocab_size = 10\n",
    "lm_head = nn.Linear(emb_dim, vocab_size, bias=False)\n",
    "\n",
    "# Get logits\n",
    "logits = lm_head(normalized_embedding)\n",
    "print(\"Logits:\", logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7206259-3c5d-4ff2-b8d3-5389071db031",
   "metadata": {},
   "source": [
    "#### The normalized embedding is passed through a linear layer that projects it from embedding space to vocabulary space (size 10 in this toy example). The result is a vector of logits, which are raw scores — one for each vocabulary tok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c859e9-5971-4a61-a6b8-95f62d8d2f7b",
   "metadata": {},
   "source": [
    "#### Step 5: Convert Logits to Probabilities using Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d639e13e-8826-45e1-9276-7140cc947f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities: tensor([0.0787, 0.1136, 0.0652, 0.2017, 0.1019, 0.0897, 0.1511, 0.0758, 0.0785,\n",
      "        0.0439], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Apply softmax to get token probabilities\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "print(\"Probabilities:\", probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15254580-873f-4e64-becd-7c0ba4872ae2",
   "metadata": {},
   "source": [
    "#### The softmax function converts the raw logits into probabilities. Each value is now between 0 and 1, and the sum of all values is 1. This gives us a distribution over the vocabulary — i.e., how likely each token is to be the next token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04656cc7-6c38-44f4-a3a4-4f84bcefe070",
   "metadata": {},
   "source": [
    "#### Step 6: Get Predicted Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f43cf1d-960e-4592-8ba6-878320644ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token index: 3\n"
     ]
    }
   ],
   "source": [
    "# Get token with highest probability\n",
    "predicted_token = torch.argmax(probs).item()\n",
    "print(f\"Predicted token index: {predicted_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c962fe-532b-4c19-a90d-f9a1d14a720f",
   "metadata": {},
   "source": [
    "#### We pick the token index with the highest probability — this is the model’s prediction for the next token in the sequence. In a real model, we would use this to generate text autoregressively.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
